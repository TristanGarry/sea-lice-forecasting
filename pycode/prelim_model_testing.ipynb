{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top-level imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard module is not an IPython extension.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import packages - setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/EEB498/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/anaconda3/envs/EEB498/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/anaconda3/envs/EEB498/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/anaconda3/envs/EEB498/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/anaconda3/envs/EEB498/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/anaconda3/envs/EEB498/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from scipy import stats\n",
    "from sklearn import metrics\n",
    "import datetime\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BroughtonSeaLice_fishData.csv',\n",
       " 'BroughtonSeaLice_fishInfo.csv',\n",
       " 'BroughtonSeaLice_siteData.csv',\n",
       " 'BroughtonSeaLice_siteInfo.csv',\n",
       " 'DFOSeaLice_Data.csv',\n",
       " 'DFOSeaLice_Info.csv',\n",
       " 'IndustrySeaLice_Data.csv',\n",
       " 'IndustrySeaLice_Info.csv',\n",
       " 'README.md']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files = os.listdir(data_dir)\n",
    "data_files.sort()\n",
    "data_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wild data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/EEB498/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (31,32) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "fish_data = pd.read_csv(f'{data_dir}BroughtonSeaLice_fishData.csv', encoding='ISO-8859-1')\n",
    "site_data = pd.read_csv(f'{data_dir}BroughtonSeaLice_siteData.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_info = pd.read_csv(f'{data_dir}BroughtonSeaLice_fishInfo.csv', encoding='ISO-8859-1')\n",
    "site_info = pd.read_csv(f'{data_dir}BroughtonSeaLice_siteInfo.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Farm data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfo_data = pd.read_csv(f'{data_dir}DFOSeaLice_Data.csv')\n",
    "dfo_info = pd.read_csv(f'{data_dir}DFOSeaLice_Info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "industry_data = pd.read_csv(f'{data_dir}IndustrySeaLice_Data.csv', encoding='ISO-8859-1', low_memory=False)\n",
    "industry_info = pd.read_csv(f'{data_dir}IndustrySeaLice_Info.csv', encoding='ISO-8859-1', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data/Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible input scenarios\n",
    "- 2001-2018: We have to trust that the model can work with the large amounts of NaN values in earlier years, both in wild data and no farmed data until 2011 \n",
    "- 2003-2018: 2003 is the first year we have data starting in March\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting overall constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to set here \n",
    "- Years to analyse\n",
    "- Within-season date range\n",
    "- Accepted ranges\n",
    "- Resampling dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_years = list(range(2003, 2018))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_months = list(range(3, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dow_dict = {\n",
    "    1: 'MON',\n",
    "    2: 'TUE',\n",
    "    3: 'WED',\n",
    "    4: 'THU', \n",
    "    5: 'FRI', \n",
    "    6: 'SAT', \n",
    "    7: 'SUN'\n",
    "}\n",
    "\n",
    "def get_dow(dt_obj):\n",
    "    dow_text = dt_obj.isoweekday()\n",
    "    return(dow_dict[dow_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wild_locations = site_data['location'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unified adult count\n",
    "This is one possible response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult = fish_data[['Lep_PAmale', 'Lep_PAfemale', \n",
    "                   'Lep_male', 'Lep_gravid',\n",
    "                   'Lep_nongravid', 'unid_PA',\n",
    "                   'unid_adult']].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_data_date = pd.to_datetime(fish_data[['year', 'day', 'month']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = pd.DataFrame({'count':adult.values, \n",
    "                         'location':fish_data['location'].values,\n",
    "                         'datetime': fish_data_date})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_glacier = response[response['location'] == 'Glacier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/EEB498/lib/python3.6/site-packages/pandas/core/indexing.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "/opt/anaconda3/envs/EEB498/lib/python3.6/site-packages/pandas/core/indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n",
      "/opt/anaconda3/envs/EEB498/lib/python3.6/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "year_df_list = []\n",
    "for year in analysis_years:\n",
    "    subset = response_glacier[response_glacier['datetime'].dt.year == year]\n",
    "    subset.loc[0] = np.nan\n",
    "    subset.loc[0, 'datetime'] = datetime.datetime(year, 1, 1)\n",
    "    subset.loc[1] = np.nan\n",
    "    subset.loc[1, 'datetime'] = datetime.datetime(year, 12, 31)\n",
    "    subset.sort_values('datetime', inplace=True)\n",
    "    subset_resampled = subset.resample(f'W-{get_dow(datetime.datetime(year, 1, 1))}',\n",
    "                                       on='datetime', label='left').apply(np.nanmean).interpolate(methods='linear')\n",
    "    year_df_list.append(subset_resampled)\n",
    "Y_glacier = pd.concat(year_df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2002-12-25</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-01-01</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-01-08</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-01-15</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-01-22</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-26</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-03</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-10</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-17</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-24</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>799 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            count\n",
       "datetime         \n",
       "2002-12-25    NaN\n",
       "2003-01-01    NaN\n",
       "2003-01-08    NaN\n",
       "2003-01-15    NaN\n",
       "2003-01-22    NaN\n",
       "...           ...\n",
       "2017-11-26    0.0\n",
       "2017-12-03    0.0\n",
       "2017-12-10    0.0\n",
       "2017-12-17    0.0\n",
       "2017-12-24    0.0\n",
       "\n",
       "[799 rows x 1 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_glacier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resample by week for average\n",
    "\n",
    "This is the code I was formerly using for the resampling, it's pretty inconsistent\n",
    "\n",
    "Y_glacier = response_glacier.resample('W', on='datetime', label='left').apply(np.nanmean)\n",
    "\n",
    "## Setting up inputs - wild data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up inputs - wild data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-motile lice\n",
    "juvenile = pd.DataFrame(fish_data[['Lep_cope', 'chalA',\n",
    "                      'chalB', 'Caligus_cope',\n",
    "                      'unid_cope', 'chal_unid']].sum(axis=1)).rename({0: 'count'}, axis=1)\n",
    "juvenile['datetime'] = fish_data_date\n",
    "juvenile['location'] = fish_data['location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_juv_list = []\n",
    "for year in analysis_years:\n",
    "    subset = juvenile[juvenile['datetime'].dt.year == year]\n",
    "    for loc in wild_locations:\n",
    "        subset = subset.append({\n",
    "            'datetime': datetime.datetime(year, 1 , 1),\n",
    "            'location': loc,\n",
    "            'count': np.nan\n",
    "        }, ignore_index=True)\n",
    "        subset = subset.append({\n",
    "            'datetime': datetime.datetime(year, 12 , 31),\n",
    "            'location': loc,\n",
    "            'count': np.nan\n",
    "        }, ignore_index=True)\n",
    "    subset.sort_values('datetime', inplace=True)\n",
    "    subset_resample = subset.groupby('location').resample(f'W-{get_dow(datetime.datetime(year, 1, 1))}',\n",
    "                                                          on='datetime', label='left').apply(np.nanmean).interpolate(methods='linear')\n",
    "    year_juv_list.append(subset_resample)\n",
    "X_wild_juv = pd.concat(year_juv_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the code I was previously using to resample the juvenile counts, now outdated\n",
    "\n",
    "X_wild_juv = juvenile.groupby('location').resample('W', on='datetime', label='left').apply(np.nanmean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing dimensionality - averaging is probably okay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_data['datetime'] = pd.to_datetime(site_data[['year', 'month', 'day']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_temp_list = []\n",
    "for year in analysis_years:\n",
    "    subset = site_data.loc[(site_data['datetime'].dt.year == year), ['datetime', 'temp', 'location']]\n",
    "    for loc in wild_locations:\n",
    "        subset = subset.append({\n",
    "            'datetime': datetime.datetime(year, 1 , 1),\n",
    "            'location': loc,\n",
    "            'temp': np.nan\n",
    "        }, ignore_index=True)\n",
    "        subset = subset.append({\n",
    "            'datetime': datetime.datetime(year, 12 , 31),\n",
    "            'location': loc,\n",
    "            'temp': np.nan\n",
    "        }, ignore_index=True)\n",
    "    subset.sort_values('datetime', inplace=True)\n",
    "    subset.sort_values('datetime', inplace=True)\n",
    "    subset_resample = subset.groupby('location').resample(f'W-{get_dow(datetime.datetime(year, 1, 1))}',\n",
    "                                                          on='datetime', label='left').apply(np.nanmean).interpolate(methods='linear')\n",
    "    year_temp_list.append(subset_resample)\n",
    "X_wild_temp = pd.concat(year_temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how I used to get these inputs, now outdated\n",
    "\n",
    "X_wild_temp = site_data[['datetime', 'temp', 'location']]\n",
    "X_wild_temp = X_wild_temp.groupby('location').resample('w', on='datetime', label='left').apply(np.nanmean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_sal_list = []\n",
    "for year in analysis_years:\n",
    "    subset = site_data.loc[(site_data['datetime'].dt.year == year), ['datetime', 'salt', 'location']]\n",
    "    for loc in wild_locations:\n",
    "        subset = subset.append({\n",
    "            'datetime': datetime.datetime(year, 1 , 1),\n",
    "            'location': loc,\n",
    "            'salt': np.nan\n",
    "        }, ignore_index=True)\n",
    "        subset = subset.append({\n",
    "            'datetime': datetime.datetime(year, 12 , 31),\n",
    "            'location': loc,\n",
    "            'salt': np.nan\n",
    "        }, ignore_index=True)\n",
    "    subset.sort_values('datetime', inplace=True)\n",
    "    subset_resample = subset.groupby('location').resample(f'W-{get_dow(datetime.datetime(year, 1, 1))}',\n",
    "                                                          on='datetime', label='left').apply(np.nanmean).interpolate(method='linear')\n",
    "    year_sal_list.append(subset_resample)\n",
    "X_wild_sal = pd.concat(year_sal_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How I used to get these inputs, now outdated\n",
    "\n",
    "X_wild_sal = site_data[['datetime', 'salt', 'location']]\n",
    "X_wild_sal = X_wild_sal.groupby('location').resample('w', on='datetime', label='left').apply(np.nanmean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up inputs - farm data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are of course concerns with using the industry counts as they *may* be of very low quality (I need to look much more extensively into this data)\n",
    "    - Should look into what kind of predictive differences including/excluding these has "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs that we want\n",
    "\n",
    "44 - Sargeunts Pass\n",
    "\n",
    "41 - Doctor Islet\n",
    "\n",
    "45 - Humphrey Rock\n",
    "\n",
    "56 - Glacier Falls\n",
    "\n",
    "54 - Simoom Sound\n",
    "\n",
    "50 - Burdwood Islands\n",
    "\n",
    "53 - Sir Edmond Bay\n",
    "\n",
    "49 - Wicklow Point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Sir Edmund Bay'], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "industry_data[industry_data['Site Common Name'].str.contains('Sir')]['Site Common Name'].unique()\n",
    "# Can't find Simoom Sound??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce dimensionality:\n",
    "\n",
    "Sum farm data, implementing # of fish for some sort of area infection indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_farms = ['Sargeaunt Pass',\n",
    "                  'Doctor Islets',\n",
    "                  'Humphrey Rock',\n",
    "                  'Simoom Sound*',\n",
    "                  'Burdwood',\n",
    "                  'Glacier Falls',\n",
    "                  'Sir Edmund Bay',\n",
    "                  'Wicklow Point'\n",
    "                 ]\n",
    "relevant_farms_iterable = ['Sargeaunt Pass',\n",
    "                           'Doctor Islets',\n",
    "                           'Humphrey Rock',\n",
    "                           'Burdwood',\n",
    "                           'Glacier Falls',\n",
    "                           'Sir Edmund Bay',\n",
    "                           'Wicklow Point'\n",
    "                          ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Burdwood', 'Sir Edmund Bay', 'Doctor Islets', 'Glacier Falls',\n",
       "       'Wicklow Point', 'Sargeaunt Pass', 'Humphrey Rock',\n",
       "       'Wicklow Point - post treatment', 'Wicklow Point - pre treatment',\n",
       "       'Wicklow Point - post-treatment', 'Wicklow Point - pre-treatment'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "industry_data[industry_data['Site Common Name'].str.contains('|'.join(relevant_farms))]['Site Common Name'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From what I can tell - most of these farms don't actually have a treatment indicator\n",
    "\n",
    "Wondering if this is significant? \n",
    "\n",
    "I also can't find Simoom Sound, maybe it's here under a different name? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_examples = industry_data[industry_data['Site Common Name'] == 'Burdwood']['Comments'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how I was resampling the industry data before\n",
    "\n",
    "X_industry = relevant_farm_data[['datetime',\n",
    "                            'Site Common Name',\n",
    "                            'Average L. salmonis motiles per fish',\n",
    "                            'Average chalimus per fish']]\n",
    "X_industry = X_industry.groupby('Site Common Name').resample('W', on='datetime', label='left').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"Recent failure to control sea louse...\" paper should have all the data I need until 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/EEB498/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/opt/anaconda3/envs/EEB498/lib/python3.6/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/envs/EEB498/lib/python3.6/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "relevant_farm_data = industry_data[industry_data['Site Common Name'].str.contains('|'.join(relevant_farms))]\n",
    "\n",
    "relevant_farm_data['Day'] = 1\n",
    "month_map = {\n",
    "    'January': 1,\n",
    "    'February': 2,\n",
    "    'March': 3,\n",
    "    'April': 4,\n",
    "    'May': 5,\n",
    "    'June': 6,\n",
    "    'July': 7,\n",
    "    'August': 8,\n",
    "    'September': 9,\n",
    "    'October': 10,\n",
    "    'November': 11,\n",
    "    'December': 12\n",
    "}\n",
    "relevant_farm_data['month'] = relevant_farm_data['Month'].map(month_map)\n",
    "relevant_farm_data['datetime'] = pd.to_datetime(relevant_farm_data[['Year', 'month', 'Day']])\n",
    "\n",
    "relevant_farm_data = relevant_farm_data[relevant_farm_data['datetime'].dt.year.isin(analysis_years)]\n",
    "\n",
    "year_industry_list = []\n",
    "for year in analysis_years:\n",
    "    subset = relevant_farm_data.loc[(relevant_farm_data['datetime'].dt.year == year), \n",
    "                                   ['datetime', 'Site Common Name', 'Average L. salmonis motiles per fish']]\n",
    "    \n",
    "    for i, farm in enumerate(relevant_farms_iterable):\n",
    "        subset = subset.append({\n",
    "            'datetime': datetime.datetime(year, 1 , 1),\n",
    "            'Site Common Name': farm,\n",
    "            'Average L. salmonis motiles per fish': np.nan\n",
    "        }, ignore_index=True)\n",
    "        subset = subset.append({\n",
    "            'datetime': datetime.datetime(year, 12 , 31),\n",
    "            'Site Common Name': farm,\n",
    "            'Average L. salmonis motiles per fish': np.nan\n",
    "        }, ignore_index=True)\n",
    "            \n",
    "    subset.sort_values('datetime', inplace=True)\n",
    "    subset_resample = subset.groupby('Site Common Name').resample(f'W-{get_dow(datetime.datetime(year, 1, 1))}',\n",
    "                                                                 on='datetime', label='left').apply(np.nanmean).interpolate(methods='linear')\n",
    "    year_industry_list.append(subset_resample)\n",
    "X_industry = pd.concat(year_industry_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forming array for model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nanmean(juv_sub, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/EEB498/lib/python3.6/site-packages/ipykernel_launcher.py:11: RuntimeWarning: Mean of empty slice\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "arrays_to_stack = []\n",
    "\n",
    "for year in analysis_years:\n",
    "    juv_sub = np.nanmean(X_wild_juv[(X_wild_juv.index.get_level_values(1).year == year) & \n",
    "                         X_wild_juv.index.get_level_values(1).month.isin(analysis_months)].unstack().T.values, axis=1)\n",
    "    temp_sub = np.nanmean(X_wild_temp[(X_wild_temp.index.get_level_values(1).year == year) & \n",
    "                           X_wild_temp.index.get_level_values(1).month.isin(analysis_months)].unstack().T.values, axis=1)\n",
    "    sal_sub = np.nanmean(X_wild_sal[(X_wild_sal.index.get_level_values(1).year == year) & \n",
    "                         X_wild_sal.index.get_level_values(1).month.isin(analysis_months)].unstack().T.values, axis=1)\n",
    "    ind_sub = np.nanmean(X_industry[(X_industry.index.get_level_values(1).year == year) & \n",
    "                         X_industry.index.get_level_values(1).month.isin(analysis_months)].unstack().T.values, axis=1)\n",
    "    \n",
    "    year_array = np.column_stack((juv_sub, temp_sub, sal_sub, ind_sub))\n",
    "    arrays_to_stack.append(year_array)\n",
    "    \n",
    "test_X = np.stack(arrays_to_stack, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 17, 4)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrays_to_stack = []\n",
    "\n",
    "for year in analysis_years:\n",
    "    year_Y = Y_glacier[(Y_glacier.index.year == year) & \n",
    "                       Y_glacier.index.month.isin(analysis_months)].values\n",
    "#     if year_Y.shape[0] == 21:\n",
    "#         year_Y = np.append(year_Y, np.nan)\n",
    "#         year_Y = year_Y.reshape(22, 1)\n",
    "    arrays_to_stack.append(year_Y)\n",
    "    \n",
    "test_Y = np.stack(arrays_to_stack, axis=0)\n",
    "test_Y = test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 17, 1)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages and set up functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write normalising functions here (let's go with MinMaxScaling for future implementations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise and fill NAs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is pretty bad!\n",
    "\n",
    "This is an issue being worked on, I need to:\n",
    "* Render the data stationary\n",
    "* Use a different scaling function\n",
    "* As a QOL update - right now these cells matter heavily on the proper order of cells being executed (top to bottom) in the notebook, that's pretty bad and should be fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/EEB498/lib/python3.6/site-packages/ipykernel_launcher.py:1: RuntimeWarning: Mean of empty slice\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/opt/anaconda3/envs/EEB498/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:1666: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  keepdims=keepdims)\n"
     ]
    }
   ],
   "source": [
    "X_mean = np.nanmean(test_X, axis=1)\n",
    "for i in range(0, X_mean.shape[0]):\n",
    "    test_X[[i]] = test_X[[i]] - X_mean[[i]]\n",
    "X_std = np.nanstd(test_X, axis=1)\n",
    "for i in range(0, X_std.shape[0]):\n",
    "    test_X[[i]] = test_X[[i]] - X_std[[i]]\n",
    "test_X = np.nan_to_num(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_mean = np.nanmean(test_Y, axis=1)\n",
    "for i in range(0, Y_mean.shape[0]):\n",
    "    test_Y[[i]] = test_Y[[i]] - Y_mean[[i]]\n",
    "Y_std = np.nanstd(test_Y, axis=1)\n",
    "for i in range(0, Y_std.shape[0]):\n",
    "    test_Y[[i]] = test_Y[[i]] - Y_std[[i]]\n",
    "test_Y = np.nan_to_num(test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = test_X[:-3]\n",
    "train_Y = test_Y[:-3]\n",
    "\n",
    "eval_X = test_X[-3:]\n",
    "eval_Y = test_Y[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive seasonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_seasonal(year: int, reference_data: pd.DataFrame):\n",
    "    '''\n",
    "    Naive seasonal model\n",
    "    This model takes a year to be predicted in and returns the last known year's values\n",
    "    \n",
    "    year: Year to be predicted\n",
    "    reference_data: Pandas dataframe of the test/input Y data, must have a DatetimeIndex index\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    pred_subset = reference_data[reference_data.index.year == year]\n",
    "    preds = pred_subset[pred_subset.index.month.isin(analysis_months)]\n",
    "    \n",
    "    return(np.array(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-parametric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some high level notes\n",
    "- This analysis is very rough and is in no way final!!!\n",
    "- The normalising, feature engineering, etc. is probably the roughest part of all of this. I don't expect it to have an effect on model choice but by no means should the input/output data be taken verbatim as what I intend to use\n",
    "- Optimizers: some reading has shown that RMSprop is the suggested optimiser for RNNs, this also coincides with François Chollet's use of optimisers so for now I am going with this one for RNNs and ADAM as the default for all others. This requires further research!\n",
    "- Reference points: \n",
    "     - ARIMA\n",
    "     - Mean & SD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up tensorboard logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long short-term memory (LSTM)\n",
    "\n",
    "This model is the most advanced RNN for sequential data, so the highest potential upside in gaining value from features but may also be overkill so make sure to compare it to some other RNN baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./logs/ \n",
    "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9 samples, validate on 3 samples\n",
      "WARNING:tensorflow:From /opt/anaconda3/envs/EEB498/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "9/9 [==============================] - 1s 82ms/sample - loss: 0.7108 - acc: 0.1307 - val_loss: 0.0463 - val_acc: 0.2353\n",
      "Epoch 2/20\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 0.4744 - acc: 0.1307 - val_loss: 0.0171 - val_acc: 0.2353\n",
      "Epoch 3/20\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 0.3560 - acc: 0.1307 - val_loss: 0.0189 - val_acc: 0.2353\n",
      "Epoch 4/20\n",
      "9/9 [==============================] - 0s 3ms/sample - loss: 0.2941 - acc: 0.1307 - val_loss: 0.0237 - val_acc: 0.2353\n",
      "Epoch 5/20\n",
      "9/9 [==============================] - 0s 3ms/sample - loss: 0.3043 - acc: 0.1307 - val_loss: 0.0352 - val_acc: 0.2353\n",
      "Epoch 6/20\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 0.2668 - acc: 0.1307 - val_loss: 0.0287 - val_acc: 0.2353\n",
      "Epoch 7/20\n",
      "9/9 [==============================] - 0s 3ms/sample - loss: 0.2224 - acc: 0.1307 - val_loss: 0.0301 - val_acc: 0.2353\n",
      "Epoch 8/20\n",
      "9/9 [==============================] - 0s 3ms/sample - loss: 0.2757 - acc: 0.1307 - val_loss: 0.0364 - val_acc: 0.2353\n",
      "Epoch 9/20\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 0.2226 - acc: 0.1307 - val_loss: 0.0277 - val_acc: 0.2353\n",
      "Epoch 10/20\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 0.2207 - acc: 0.1307 - val_loss: 0.0275 - val_acc: 0.2353\n",
      "Epoch 11/20\n",
      "9/9 [==============================] - 0s 3ms/sample - loss: 0.1818 - acc: 0.1307 - val_loss: 0.0269 - val_acc: 0.2353\n",
      "Epoch 12/20\n",
      "9/9 [==============================] - 0s 3ms/sample - loss: 0.1845 - acc: 0.1307 - val_loss: 0.0272 - val_acc: 0.2353\n",
      "Epoch 13/20\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 0.1604 - acc: 0.1307 - val_loss: 0.0266 - val_acc: 0.2353\n",
      "Epoch 14/20\n",
      "9/9 [==============================] - 0s 3ms/sample - loss: 0.1589 - acc: 0.1307 - val_loss: 0.0242 - val_acc: 0.2353\n",
      "Epoch 15/20\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 0.1783 - acc: 0.1307 - val_loss: 0.0256 - val_acc: 0.2353\n",
      "Epoch 16/20\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 0.1714 - acc: 0.1307 - val_loss: 0.0277 - val_acc: 0.2353\n",
      "Epoch 17/20\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 0.1403 - acc: 0.1307 - val_loss: 0.0268 - val_acc: 0.2353\n",
      "Epoch 18/20\n",
      "9/9 [==============================] - 0s 3ms/sample - loss: 0.1483 - acc: 0.1307 - val_loss: 0.0288 - val_acc: 0.2353\n",
      "Epoch 19/20\n",
      "9/9 [==============================] - 0s 2ms/sample - loss: 0.1461 - acc: 0.1307 - val_loss: 0.0275 - val_acc: 0.2353\n",
      "Epoch 20/20\n",
      "9/9 [==============================] - 0s 3ms/sample - loss: 0.1420 - acc: 0.1307 - val_loss: 0.0297 - val_acc: 0.2353\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.LSTM(100, input_shape=(17, 4),\n",
    "               return_sequences=True,\n",
    "               activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer='rmsprop', \n",
    "              loss='mse',\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(train_X, train_Y, \n",
    "                    epochs=20, batch_size=16,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = model.predict(train_X)\n",
    "\n",
    "# Train RMSE\n",
    "metrics.mean_squared_error(train_Y.reshape(train_Y.shape[0], train_Y.shape[1] * train_Y.shape[2]),\n",
    "                           train_predictions.reshape(train_predictions.shape[0], train_predictions.shape[1] * train_predictions.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(eval_X)\n",
    "\n",
    "# Test RMSE\n",
    "metrics.mean_squared_error(eval_Y.reshape(eval_Y.shape[0], eval_Y.shape[1] * eval_Y.shape[2]),\n",
    "                           test_predictions.reshape(test_predictions.shape[0], test_predictions.shape[1] * train_predictions.shape[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1D Convnet\n",
    "\n",
    "1D convnets have sometimes shown to be more efficient with data than similarly sized RNNs when dealing with small problems. Maybe that means they're a good application here? Each output timestep takes power from its neighbours, so this is assuming some sort of autoregression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 17, 1)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 204 into shape (12,22)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-07a3c2d37a74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m               metrics=['acc'])\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m history = model.fit(train_X, train_Y.reshape(12, 22), \n\u001b[0m\u001b[1;32m     16\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     validation_split=0.2)\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 204 into shape (12,22)"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Conv1D(filters=32, kernel_size=2,\n",
    "                        activation='relu',\n",
    "                        input_shape=(22,6)))\n",
    "model.add(layers.MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(50, activation='relu'))\n",
    "model.add(layers.Dense(1))\n",
    "# 1D Convnets cannot return sequences, they converge to one value...\n",
    "model.compile(optimizer='adam', \n",
    "              loss='mse',\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(train_X, train_Y.reshape(12, 22), \n",
    "                    epochs=100, batch_size=16,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EEB498",
   "language": "python",
   "name": "eeb498"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
