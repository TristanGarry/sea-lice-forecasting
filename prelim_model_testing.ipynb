{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top-level imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import packages - setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from scipy import stats\n",
    "from sklearn import metrics\n",
    "import datetime\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BroughtonSeaLice_fishData.csv',\n",
       " 'BroughtonSeaLice_fishInfo.csv',\n",
       " 'BroughtonSeaLice_siteData.csv',\n",
       " 'BroughtonSeaLice_siteInfo.csv',\n",
       " 'DFOSeaLice_Data.csv',\n",
       " 'DFOSeaLice_Info.csv',\n",
       " 'IndustrySeaLice_Data.csv',\n",
       " 'IndustrySeaLice_Info.csv',\n",
       " 'README.md']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files = os.listdir(data_dir)\n",
    "data_files.sort()\n",
    "data_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wild data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/ipython/7.8.0/libexec/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (31,32) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "fish_data = pd.read_csv(f'{data_dir}BroughtonSeaLice_fishData.csv', encoding='ISO-8859-1')\n",
    "site_data = pd.read_csv(f'{data_dir}BroughtonSeaLice_siteData.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_info = pd.read_csv(f'{data_dir}BroughtonSeaLice_fishInfo.csv', encoding='ISO-8859-1')\n",
    "site_info = pd.read_csv(f'{data_dir}BroughtonSeaLice_siteInfo.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Farm data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfo_data = pd.read_csv(f'{data_dir}DFOSeaLice_Data.csv')\n",
    "dfo_info = pd.read_csv(f'{data_dir}DFOSeaLice_Info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "industry_data = pd.read_csv(f'{data_dir}IndustrySeaLice_Data.csv', encoding='ISO-8859-1', low_memory=False)\n",
    "industry_info = pd.read_csv(f'{data_dir}IndustrySeaLice_Info.csv', encoding='ISO-8859-1', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data/Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible input scenarios\n",
    "- 2001-2018: We have to trust that the model can work with the large amounts of NaN values in earlier years, both in wild data and no farmed data until 2011 \n",
    "- 2003-2018: 2003 is the first year we have data starting in March\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting overall constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to set here \n",
    "- Years to analyse\n",
    "- Within-season date range\n",
    "- Linear scaling factors (*return to this*)\n",
    "- Accepted ranges\n",
    "- Resampling dates (*return to this*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_years = list(range(2003, 2018))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_months = list(range(3, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unified adult count\n",
    "This is one possible response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['fish_id', 'site_id', 'year', 'day', 'month', 'location', 'fish_num',\n",
       "       'species', 'length', 'height', 'Lep_cope', 'chalA', 'chalB',\n",
       "       'Lep_PAmale', 'Lep_PAfemale', 'Lep_male', 'Lep_nongravid', 'Lep_gravid',\n",
       "       'Caligus_cope', 'Caligus_mot', 'Caligus_gravid', 'unid_cope',\n",
       "       'chal_unid', 'unid_PA', 'unid_adult', 'chal_scar', 'pred_scar',\n",
       "       'mot_scar', 'hem', 'mateguarding', 'eroded_gill', 'white_eye',\n",
       "       'blue_blotches', 'pinched_belly', 'scales', 'comments'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fish_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult = fish_data[['Lep_PAmale', 'Lep_PAfemale', \n",
    "                   'Lep_male', 'Lep_gravid',\n",
    "                   'Lep_nongravid', 'unid_PA',\n",
    "                   'unid_adult']].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_data_date = pd.to_datetime(fish_data[['year', 'day', 'month']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = pd.DataFrame({'count':adult.values, \n",
    "                         'location':fish_data['location'].values,\n",
    "                         'datetime': fish_data_date})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_glacier = response[response['location'] == 'Glacier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample by week for average\n",
    "Y_glacier = response_glacier.resample('W', on='datetime', label='left').apply(np.nanmean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up inputs - wild data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-motile lice\n",
    "juvenile = pd.DataFrame(fish_data[['Lep_cope', 'chalA',\n",
    "                      'chalB', 'Caligus_cope',\n",
    "                      'unid_cope', 'chal_unid']].sum(axis=1)).rename({0: 'count'}, axis=1)\n",
    "juvenile['datetime'] = fish_data_date\n",
    "juvenile['location'] = fish_data['location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_wild_juv = juvenile.groupby('location').resample('W', on='datetime', label='left').apply(np.nanmean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temperature\n",
    "site_data['datetime'] = pd.to_datetime(site_data[['year', 'month', 'day']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_wild_temp = site_data[['datetime', 'temp', 'location']]\n",
    "X_wild_temp = X_wild_temp.groupby('location').resample('w', on='datetime', label='left').apply(np.nanmean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up inputs - farm data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Implementing treatments *may* not be a priority - I'll have to do some extensive testing on this\n",
    "- There are of course concerns with using the industry counts as they *may* be of very low quality (I need to look much more extensively into this data)\n",
    "    - Should look into what kind of predictive differences including/excluding these has "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "industry_data['Day'] = 1\n",
    "month_map = {\n",
    "    'January': 1,\n",
    "    'February': 2,\n",
    "    'March': 3,\n",
    "    'April': 4,\n",
    "    'May': 5,\n",
    "    'June': 6,\n",
    "    'July': 7,\n",
    "    'August': 8,\n",
    "    'September': 9,\n",
    "    'October': 10,\n",
    "    'November': 11,\n",
    "    'December': 12\n",
    "}\n",
    "industry_data['month'] = industry_data['Month'].map(month_map)\n",
    "industry_data['datetime'] = pd.to_datetime(industry_data[['Year', 'month', 'Day']])\n",
    "\n",
    "X_industry = industry_data[['datetime',\n",
    "                            'Site Common Name',\n",
    "                            'Average L. salmonis motiles per fish',\n",
    "                            'Average chalimus per fish']]\n",
    "X_industry = X_industry.groupby('Site Common Name').resample('W', on='datetime', label='left').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forming array for model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrays_to_stack = []\n",
    "\n",
    "for year in analysis_years:\n",
    "    juv_sub = X_wild_juv[(X_wild_juv.index.get_level_values(1).year == year) & \n",
    "                         X_wild_juv.index.get_level_values(1).month.isin(analysis_months)].unstack().T.values\n",
    "    temp_sub = X_wild_temp[(X_wild_temp.index.get_level_values(1).year == year) & \n",
    "                           X_wild_temp.index.get_level_values(1).month.isin(analysis_months)].unstack().T.values\n",
    "    \n",
    "    year_array = np.concatenate((juv_sub, temp_sub), axis=1)\n",
    "    if year_array.shape[0] == 21:\n",
    "        year_array = np.append(year_array, np.full(6, np.nan).reshape(1, 6), axis=0)\n",
    "    arrays_to_stack.append(year_array)\n",
    "    \n",
    "test_X = np.stack(arrays_to_stack, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 22, 6)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrays_to_stack = []\n",
    "\n",
    "for year in analysis_years:\n",
    "    year_Y = Y_glacier[(Y_glacier.index.year == year) & \n",
    "                       Y_glacier.index.month.isin(analysis_months)].values\n",
    "    if year_Y.shape[0] == 21:\n",
    "        year_Y = np.append(year_Y, np.nan)\n",
    "        year_Y = year_Y.reshape(22, 1)\n",
    "    arrays_to_stack.append(year_Y)\n",
    "    \n",
    "test_Y = np.stack(arrays_to_stack, axis=0)\n",
    "test_Y = test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 22, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages and set up functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write normalising functions here (let's go with MinMaxScaling for future implementations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise and fill NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mean = np.nanmean(test_X, axis=1)\n",
    "for i in range(0, X_mean.shape[0]):\n",
    "    test_X[[i]] = test_X[[i]] - X_mean[[i]]\n",
    "X_std = np.nanstd(test_X, axis=1)\n",
    "for i in range(0, X_std.shape[0]):\n",
    "    test_X[[i]] = test_X[[i]] - X_std[[i]]\n",
    "test_X = np.nan_to_num(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_mean = np.nanmean(test_Y, axis=1)\n",
    "for i in range(0, Y_mean.shape[0]):\n",
    "    test_Y[[i]] = test_Y[[i]] - Y_mean[[i]]\n",
    "Y_std = np.nanstd(test_Y, axis=1)\n",
    "for i in range(0, Y_std.shape[0]):\n",
    "    test_Y[[i]] = test_Y[[i]] - Y_std[[i]]\n",
    "test_Y = np.nan_to_num(test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = test_X[:-3]\n",
    "train_Y = test_Y[:-3]\n",
    "\n",
    "eval_X = test_X[-3:]\n",
    "eval_Y = test_Y[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive seasonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_seasonal(year: int, reference_data: pd.DataFrame):\n",
    "    '''\n",
    "    Naive seasonal model\n",
    "    This model takes a year to be predicted in and returns the last known year's values\n",
    "    \n",
    "    year: Year to be predicted\n",
    "    reference_data: Pandas dataframe of the test/input Y data, must have a DatetimeIndex index\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    pred_subset = reference_data[reference_data.index.year == year]\n",
    "    preds = pred_subset[pred_subset.index.month.isin(analysis_months)]\n",
    "    \n",
    "    return(np.array(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-parametric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some high level notes\n",
    "- This analysis is very rough and is in no way final!!!\n",
    "- The normalising, feature engineering, etc. is probably the roughest part of all of this. I don't expect it to have an effect on model choice but by no means should the input/output data be taken verbatim as what I intend to use\n",
    "- Optimizers: some reading has shown that RMSprop is the suggested optimiser for RNNs, this also coincides with François Chollet's use of optimisers so for now I am going with this one for RNNs and ADAM as the default for all others. This requires further research!\n",
    "- Reference points: \n",
    "     - ARIMA\n",
    "     - Mean & SD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up tensorboard logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long short-term memory (LSTM)\n",
    "\n",
    "This model is the most advanced RNN for sequential data, so the highest potential upside in gaining value from features but may also be overkill so make sure to compare it to some other RNN baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./logs/ \n",
    "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9 samples, validate on 3 samples\n",
      "Epoch 1/100\n",
      "9/9 [==============================] - 4s 431ms/sample - loss: 0.5583 - acc: 0.4848 - val_loss: 0.0233 - val_acc: 0.4394\n",
      "Epoch 2/100\n",
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.520728). Check your callbacks.\n",
      "9/9 [==============================] - 1s 70ms/sample - loss: 0.4975 - acc: 0.4848 - val_loss: 0.0040 - val_acc: 0.4394\n",
      "Epoch 3/100\n",
      "9/9 [==============================] - 0s 8ms/sample - loss: 0.4732 - acc: 0.4848 - val_loss: 0.0048 - val_acc: 0.4394\n",
      "Epoch 4/100\n",
      "9/9 [==============================] - 0s 8ms/sample - loss: 0.4373 - acc: 0.4848 - val_loss: 0.0044 - val_acc: 0.4394\n",
      "Epoch 5/100\n",
      "9/9 [==============================] - 0s 8ms/sample - loss: 0.3779 - acc: 0.4848 - val_loss: 0.0029 - val_acc: 0.4394\n",
      "Epoch 6/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.3258 - acc: 0.4697 - val_loss: 0.0089 - val_acc: 0.4394\n",
      "Epoch 7/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 1.9517 - acc: 0.4545 - val_loss: 0.0039 - val_acc: 0.4394\n",
      "Epoch 8/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.4167 - acc: 0.4646 - val_loss: 0.0030 - val_acc: 0.4394\n",
      "Epoch 9/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.3624 - acc: 0.4798 - val_loss: 0.0034 - val_acc: 0.4394\n",
      "Epoch 10/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.3408 - acc: 0.4848 - val_loss: 0.0034 - val_acc: 0.4394\n",
      "Epoch 11/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.3335 - acc: 0.4848 - val_loss: 0.0042 - val_acc: 0.4394\n",
      "Epoch 12/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.3212 - acc: 0.4848 - val_loss: 0.0036 - val_acc: 0.4394\n",
      "Epoch 13/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.2623 - acc: 0.4848 - val_loss: 0.0038 - val_acc: 0.4394\n",
      "Epoch 14/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.2599 - acc: 0.4798 - val_loss: 0.0032 - val_acc: 0.4394\n",
      "Epoch 15/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.2299 - acc: 0.4545 - val_loss: 0.0098 - val_acc: 0.4394\n",
      "Epoch 16/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.3611 - acc: 0.4848 - val_loss: 0.0030 - val_acc: 0.4394\n",
      "Epoch 17/100\n",
      "9/9 [==============================] - 0s 6ms/sample - loss: 0.2558 - acc: 0.4646 - val_loss: 0.0034 - val_acc: 0.4394\n",
      "Epoch 18/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.2448 - acc: 0.4848 - val_loss: 0.0034 - val_acc: 0.4394\n",
      "Epoch 19/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.2297 - acc: 0.4848 - val_loss: 0.0036 - val_acc: 0.4394\n",
      "Epoch 20/100\n",
      "9/9 [==============================] - 0s 6ms/sample - loss: 0.2126 - acc: 0.4848 - val_loss: 0.0036 - val_acc: 0.4394\n",
      "Epoch 21/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.2106 - acc: 0.4798 - val_loss: 0.0039 - val_acc: 0.4394\n",
      "Epoch 22/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.1791 - acc: 0.4747 - val_loss: 0.0037 - val_acc: 0.4394\n",
      "Epoch 23/100\n",
      "9/9 [==============================] - 0s 6ms/sample - loss: 0.2012 - acc: 0.4747 - val_loss: 0.0043 - val_acc: 0.4394\n",
      "Epoch 24/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.2153 - acc: 0.4848 - val_loss: 0.0043 - val_acc: 0.4394\n",
      "Epoch 25/100\n",
      "9/9 [==============================] - 0s 6ms/sample - loss: 0.2063 - acc: 0.4646 - val_loss: 0.0036 - val_acc: 0.4394\n",
      "Epoch 26/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.1717 - acc: 0.4848 - val_loss: 0.0035 - val_acc: 0.4394\n",
      "Epoch 27/100\n",
      "9/9 [==============================] - 0s 6ms/sample - loss: 0.1764 - acc: 0.4747 - val_loss: 0.0034 - val_acc: 0.4394\n",
      "Epoch 28/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.1762 - acc: 0.4848 - val_loss: 0.0038 - val_acc: 0.4394\n",
      "Epoch 29/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.1936 - acc: 0.4798 - val_loss: 0.0039 - val_acc: 0.4394\n",
      "Epoch 30/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.2048 - acc: 0.4697 - val_loss: 0.0043 - val_acc: 0.4394\n",
      "Epoch 31/100\n",
      "9/9 [==============================] - 0s 6ms/sample - loss: 0.1979 - acc: 0.4848 - val_loss: 0.0034 - val_acc: 0.4394\n",
      "Epoch 32/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.2127 - acc: 0.4848 - val_loss: 0.0039 - val_acc: 0.4394\n",
      "Epoch 33/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.1671 - acc: 0.4798 - val_loss: 0.0041 - val_acc: 0.4394\n",
      "Epoch 34/100\n",
      "9/9 [==============================] - 0s 6ms/sample - loss: 0.2008 - acc: 0.4747 - val_loss: 0.0040 - val_acc: 0.4394\n",
      "Epoch 35/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.1540 - acc: 0.4798 - val_loss: 0.0043 - val_acc: 0.4394\n",
      "Epoch 36/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.1484 - acc: 0.4848 - val_loss: 0.0038 - val_acc: 0.4394\n",
      "Epoch 37/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.1759 - acc: 0.4798 - val_loss: 0.0040 - val_acc: 0.4394\n",
      "Epoch 38/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.1420 - acc: 0.4798 - val_loss: 0.0041 - val_acc: 0.4394\n",
      "Epoch 39/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.1350 - acc: 0.4848 - val_loss: 0.0049 - val_acc: 0.4394\n",
      "Epoch 40/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.1508 - acc: 0.4798 - val_loss: 0.0042 - val_acc: 0.4394\n",
      "Epoch 41/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.1457 - acc: 0.4848 - val_loss: 0.0055 - val_acc: 0.4394\n",
      "Epoch 42/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.1387 - acc: 0.4697 - val_loss: 0.0041 - val_acc: 0.4394\n",
      "Epoch 43/100\n",
      "9/9 [==============================] - 0s 6ms/sample - loss: 0.1365 - acc: 0.4747 - val_loss: 0.0042 - val_acc: 0.4394\n",
      "Epoch 44/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.1634 - acc: 0.4798 - val_loss: 0.0036 - val_acc: 0.4394\n",
      "Epoch 45/100\n",
      "9/9 [==============================] - 0s 6ms/sample - loss: 0.1652 - acc: 0.4848 - val_loss: 0.0043 - val_acc: 0.4394\n",
      "Epoch 46/100\n",
      "9/9 [==============================] - 0s 6ms/sample - loss: 0.1427 - acc: 0.4848 - val_loss: 0.0051 - val_acc: 0.4394\n",
      "Epoch 47/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.1704 - acc: 0.4747 - val_loss: 0.0069 - val_acc: 0.4394\n",
      "Epoch 48/100\n",
      "9/9 [==============================] - 0s 6ms/sample - loss: 0.1611 - acc: 0.4798 - val_loss: 0.0053 - val_acc: 0.4394\n",
      "Epoch 49/100\n",
      "9/9 [==============================] - 0s 6ms/sample - loss: 0.1497 - acc: 0.4848 - val_loss: 0.0046 - val_acc: 0.4394\n",
      "Epoch 50/100\n",
      "9/9 [==============================] - 0s 6ms/sample - loss: 0.1215 - acc: 0.4848 - val_loss: 0.0051 - val_acc: 0.4394\n",
      "Epoch 51/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.1173 - acc: 0.4697 - val_loss: 0.0062 - val_acc: 0.4394\n",
      "Epoch 52/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.1665 - acc: 0.4848 - val_loss: 0.0071 - val_acc: 0.4394\n",
      "Epoch 53/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.1973 - acc: 0.4798 - val_loss: 0.0041 - val_acc: 0.4394\n",
      "Epoch 54/100\n",
      "9/9 [==============================] - 0s 6ms/sample - loss: 0.1321 - acc: 0.4848 - val_loss: 0.0043 - val_acc: 0.4394\n",
      "Epoch 55/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.1360 - acc: 0.4798 - val_loss: 0.0047 - val_acc: 0.4394\n",
      "Epoch 56/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.1477 - acc: 0.4747 - val_loss: 0.0053 - val_acc: 0.4394\n",
      "Epoch 57/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.1278 - acc: 0.4848 - val_loss: 0.0047 - val_acc: 0.4394\n",
      "Epoch 58/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.1241 - acc: 0.4798 - val_loss: 0.0048 - val_acc: 0.4394\n",
      "Epoch 59/100\n",
      "9/9 [==============================] - 0s 6ms/sample - loss: 0.1274 - acc: 0.4798 - val_loss: 0.0050 - val_acc: 0.4394\n",
      "Epoch 60/100\n",
      "9/9 [==============================] - 0s 6ms/sample - loss: 0.1575 - acc: 0.4848 - val_loss: 0.0043 - val_acc: 0.4394\n",
      "Epoch 61/100\n",
      "9/9 [==============================] - 0s 6ms/sample - loss: 0.1105 - acc: 0.4848 - val_loss: 0.0044 - val_acc: 0.4394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100\n",
      "9/9 [==============================] - 0s 6ms/sample - loss: 0.1014 - acc: 0.4798 - val_loss: 0.0045 - val_acc: 0.4394\n",
      "Epoch 63/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.1155 - acc: 0.4848 - val_loss: 0.0048 - val_acc: 0.4394\n",
      "Epoch 64/100\n",
      "9/9 [==============================] - 0s 6ms/sample - loss: 0.0823 - acc: 0.4798 - val_loss: 0.0044 - val_acc: 0.4394\n",
      "Epoch 65/100\n",
      "9/9 [==============================] - 0s 6ms/sample - loss: 0.1147 - acc: 0.4848 - val_loss: 0.0054 - val_acc: 0.4394\n",
      "Epoch 66/100\n",
      "9/9 [==============================] - 0s 6ms/sample - loss: 0.1087 - acc: 0.4798 - val_loss: 0.0071 - val_acc: 0.4394\n",
      "Epoch 67/100\n",
      "9/9 [==============================] - 0s 6ms/sample - loss: 0.1372 - acc: 0.4848 - val_loss: 0.0058 - val_acc: 0.4394\n",
      "Epoch 68/100\n",
      "9/9 [==============================] - 0s 6ms/sample - loss: 0.1074 - acc: 0.4798 - val_loss: 0.0048 - val_acc: 0.4394\n",
      "Epoch 69/100\n",
      "9/9 [==============================] - 0s 6ms/sample - loss: 0.1183 - acc: 0.4848 - val_loss: 0.0056 - val_acc: 0.4394\n",
      "Epoch 70/100\n",
      "9/9 [==============================] - 0s 6ms/sample - loss: 0.0979 - acc: 0.4798 - val_loss: 0.0052 - val_acc: 0.4394\n",
      "Epoch 71/100\n",
      "9/9 [==============================] - 0s 6ms/sample - loss: 0.0918 - acc: 0.4848 - val_loss: 0.0064 - val_acc: 0.4394\n",
      "Epoch 72/100\n",
      "9/9 [==============================] - 0s 6ms/sample - loss: 0.1008 - acc: 0.4798 - val_loss: 0.0063 - val_acc: 0.4394\n",
      "Epoch 73/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.1131 - acc: 0.4848 - val_loss: 0.0045 - val_acc: 0.4394\n",
      "Epoch 74/100\n",
      "9/9 [==============================] - 0s 6ms/sample - loss: 0.0757 - acc: 0.4798 - val_loss: 0.0050 - val_acc: 0.4394\n",
      "Epoch 75/100\n",
      "9/9 [==============================] - 0s 6ms/sample - loss: 0.0909 - acc: 0.4798 - val_loss: 0.0048 - val_acc: 0.4394\n",
      "Epoch 76/100\n",
      "9/9 [==============================] - 0s 6ms/sample - loss: 0.0736 - acc: 0.4848 - val_loss: 0.0052 - val_acc: 0.4394\n",
      "Epoch 77/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.0920 - acc: 0.4798 - val_loss: 0.0051 - val_acc: 0.4394\n",
      "Epoch 78/100\n",
      "9/9 [==============================] - 0s 6ms/sample - loss: 0.0939 - acc: 0.4848 - val_loss: 0.0046 - val_acc: 0.4394\n",
      "Epoch 79/100\n",
      "9/9 [==============================] - 0s 6ms/sample - loss: 0.0929 - acc: 0.4848 - val_loss: 0.0048 - val_acc: 0.4394\n",
      "Epoch 80/100\n",
      "9/9 [==============================] - 0s 6ms/sample - loss: 0.0803 - acc: 0.4848 - val_loss: 0.0053 - val_acc: 0.4394\n",
      "Epoch 81/100\n",
      "9/9 [==============================] - 0s 8ms/sample - loss: 0.0845 - acc: 0.4798 - val_loss: 0.0049 - val_acc: 0.4394\n",
      "Epoch 82/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.0745 - acc: 0.4848 - val_loss: 0.0057 - val_acc: 0.4394\n",
      "Epoch 83/100\n",
      "9/9 [==============================] - 0s 6ms/sample - loss: 0.0914 - acc: 0.4798 - val_loss: 0.0051 - val_acc: 0.4394\n",
      "Epoch 84/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.1097 - acc: 0.4798 - val_loss: 0.0057 - val_acc: 0.4394\n",
      "Epoch 85/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.0665 - acc: 0.4848 - val_loss: 0.0045 - val_acc: 0.4394\n",
      "Epoch 86/100\n",
      "9/9 [==============================] - 0s 6ms/sample - loss: 0.0924 - acc: 0.4848 - val_loss: 0.0056 - val_acc: 0.4394\n",
      "Epoch 87/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.0721 - acc: 0.4848 - val_loss: 0.0066 - val_acc: 0.4394\n",
      "Epoch 88/100\n",
      "9/9 [==============================] - 0s 6ms/sample - loss: 0.1490 - acc: 0.4848 - val_loss: 0.0071 - val_acc: 0.4394\n",
      "Epoch 89/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.1296 - acc: 0.4848 - val_loss: 0.0041 - val_acc: 0.4394\n",
      "Epoch 90/100\n",
      "9/9 [==============================] - 0s 6ms/sample - loss: 0.0868 - acc: 0.4848 - val_loss: 0.0044 - val_acc: 0.4394\n",
      "Epoch 91/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.1015 - acc: 0.4848 - val_loss: 0.0045 - val_acc: 0.4394\n",
      "Epoch 92/100\n",
      "9/9 [==============================] - 0s 6ms/sample - loss: 0.0871 - acc: 0.4848 - val_loss: 0.0049 - val_acc: 0.4394\n",
      "Epoch 93/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.0852 - acc: 0.4848 - val_loss: 0.0057 - val_acc: 0.4394\n",
      "Epoch 94/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.0686 - acc: 0.4848 - val_loss: 0.0060 - val_acc: 0.4394\n",
      "Epoch 95/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.0575 - acc: 0.4848 - val_loss: 0.0052 - val_acc: 0.4394\n",
      "Epoch 96/100\n",
      "9/9 [==============================] - 0s 6ms/sample - loss: 0.0621 - acc: 0.4798 - val_loss: 0.0052 - val_acc: 0.4394\n",
      "Epoch 97/100\n",
      "9/9 [==============================] - 0s 6ms/sample - loss: 0.0578 - acc: 0.4848 - val_loss: 0.0048 - val_acc: 0.4394\n",
      "Epoch 98/100\n",
      "9/9 [==============================] - 0s 7ms/sample - loss: 0.0510 - acc: 0.4848 - val_loss: 0.0048 - val_acc: 0.4394\n",
      "Epoch 99/100\n",
      "9/9 [==============================] - 0s 6ms/sample - loss: 0.0560 - acc: 0.4848 - val_loss: 0.0045 - val_acc: 0.4394\n",
      "Epoch 100/100\n",
      "9/9 [==============================] - 0s 6ms/sample - loss: 0.0509 - acc: 0.4848 - val_loss: 0.0043 - val_acc: 0.4394\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.LSTM(100, input_shape=(22, 6),\n",
    "               return_sequences=True,\n",
    "               activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "model.add(tf.keras.layers.LSTM(100, input_shape=(22, 6), \n",
    "               return_sequences=True,\n",
    "               activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "model.add(tf.keras.layers.LSTM(100, input_shape=(22, 6),\n",
    "               return_sequences=True,\n",
    "               activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer='rmsprop', \n",
    "              loss='mse',\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(train_X, train_Y, \n",
    "                    epochs=100, batch_size=16,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 67499), started 0:09:15 ago. (Use '!kill 67499' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-ce8b1a8274eadd4c\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-ce8b1a8274eadd4c\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.035585599766764814"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_predictions = model.predict(train_X)\n",
    "\n",
    "# Train RMSE\n",
    "metrics.mean_squared_error(train_Y.reshape(train_Y.shape[0], train_Y.shape[1] * train_Y.shape[2]),\n",
    "                           train_predictions.reshape(train_predictions.shape[0], train_predictions.shape[1] * train_predictions.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.042329201931601945"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions = model.predict(eval_X)\n",
    "\n",
    "# Test RMSE\n",
    "metrics.mean_squared_error(eval_Y.reshape(eval_Y.shape[0], eval_Y.shape[1] * eval_Y.shape[2]),\n",
    "                           test_predictions.reshape(test_predictions.shape[0], test_predictions.shape[1] * train_predictions.shape[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1D Convnet\n",
    "\n",
    "1D convnets have sometimes shown to be more efficient with data than similarly sized RNNs when dealing with small problems. Maybe that means they're a good application here? Each output timestep takes power from its neighbours, so this is assuming some sort of autoregression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 22, 1)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_5 to have shape (1,) but got array with shape (22,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-07a3c2d37a74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m history = model.fit(train_X, train_Y.reshape(12, 22), \n\u001b[1;32m     16\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                     validation_split=0.2)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    143\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_5 to have shape (1,) but got array with shape (22,)"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Conv1D(filters=32, kernel_size=2,\n",
    "                        activation='relu',\n",
    "                        input_shape=(22,6)))\n",
    "model.add(layers.MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(50, activation='relu'))\n",
    "model.add(layers.Dense(1))\n",
    "# 1D Convnets cannot return sequences, they converge to one value...\n",
    "model.compile(optimizer='adam', \n",
    "              loss='mse',\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(train_X, train_Y.reshape(12, 22), \n",
    "                    epochs=100, batch_size=16,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
