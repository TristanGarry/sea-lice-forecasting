{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_data = pd.read_csv(f'{data_dir}BroughtonSeaLice_fishData.csv', encoding='ISO-8859-1', low_memory=False)\n",
    "site_data = pd.read_csv(f'{data_dir}BroughtonSeaLice_siteData.csv', encoding='ISO-8859-1', low_memory=False)\n",
    "industry_data = pd.read_csv(f'{data_dir}IndustrySeaLice_Data.csv', encoding='ISO-8859-1', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Constants and helpers\n",
    "analysis_years = list(range(2003, 2018))\n",
    "\n",
    "analysis_months = list(range(1, 7))\n",
    "\n",
    "dow_dict = {\n",
    "    1: 'MON',\n",
    "    2: 'TUE',\n",
    "    3: 'WED',\n",
    "    4: 'THU', \n",
    "    5: 'FRI', \n",
    "    6: 'SAT', \n",
    "    7: 'SUN'\n",
    "}\n",
    "\n",
    "month_map = {\n",
    "    'January': 1,\n",
    "    'February': 2,\n",
    "    'March': 3,\n",
    "    'April': 4,\n",
    "    'May': 5,\n",
    "    'June': 6,\n",
    "    'July': 7,\n",
    "    'August': 8,\n",
    "    'September': 9,\n",
    "    'October': 10,\n",
    "    'November': 11,\n",
    "    'December': 12\n",
    "}\n",
    "\n",
    "def get_dow(dt_obj):\n",
    "    dow_text = dt_obj.isoweekday()\n",
    "    return(dow_dict[dow_text])\n",
    "\n",
    "wild_locations = site_data['location'].unique()\n",
    "wild_locations = ['Glacier']\n",
    "\n",
    "def split_last_n_by_grain(df, n):\n",
    "    df_grouped = df.sort_values('datetime').groupby('year', group_keys=False)\n",
    "    df_head = df_grouped.apply(lambda dfg: dfg.iloc[:-n])\n",
    "    df_tail = df_grouped.apply(lambda dfg: dfg.iloc[-n:])\n",
    "    return df_head, df_tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unified adult count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/EEB498/lib/python3.6/site-packages/pandas/core/indexing.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "/opt/anaconda3/envs/EEB498/lib/python3.6/site-packages/pandas/core/indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n",
      "/opt/anaconda3/envs/EEB498/lib/python3.6/site-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "adult = fish_data[['Lep_PAmale', 'Lep_PAfemale', \n",
    "                   'Lep_male', 'Lep_gravid',\n",
    "                   'Lep_nongravid', 'unid_PA',\n",
    "                   'unid_adult']].sum(axis=1)\n",
    "\n",
    "fish_data_date = pd.to_datetime(fish_data[['year', 'day', 'month']])\n",
    "\n",
    "response = pd.DataFrame({'count':adult.values, \n",
    "                         'location':fish_data['location'].values,\n",
    "                         'datetime': fish_data_date})\n",
    "\n",
    "response_glacier = response[response['location'] == 'Glacier']\n",
    "\n",
    "\n",
    "\n",
    "year_df_list = []\n",
    "for year in analysis_years:\n",
    "    subset = response_glacier[response_glacier['datetime'].dt.year == year]\n",
    "    subset.loc[0] = np.nan\n",
    "    subset.loc[0, 'datetime'] = datetime.datetime(year, 1, 1)\n",
    "    subset.loc[1] = np.nan\n",
    "    subset.loc[1, 'datetime'] = datetime.datetime(year, 12, 31)\n",
    "    subset.sort_values('datetime', inplace=True)\n",
    "    subset_resampled = subset.resample(f'W-{get_dow(datetime.datetime(year, 1, 1))}',\n",
    "                                       on='datetime', label='left').mean().interpolate(methods='linear')\n",
    "    year_df_list.append(subset_resampled)\n",
    "Y_glacier = pd.concat(year_df_list).reset_index().set_index('datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Non-motile lice\n",
    "juvenile = pd.DataFrame(fish_data[['Lep_cope', 'chalA',\n",
    "                      'chalB', 'Caligus_cope',\n",
    "                      'unid_cope', 'chal_unid']].sum(axis=1)).rename({0: 'count'}, axis=1)\n",
    "juvenile['datetime'] = fish_data_date\n",
    "juvenile['location'] = fish_data['location']\n",
    "\n",
    "juvenile = juvenile[juvenile['location'] == 'Glacier']\n",
    "\n",
    "year_juv_list = []\n",
    "for year in analysis_years:\n",
    "    subset = juvenile[juvenile['datetime'].dt.year == year]\n",
    "    for loc in wild_locations:\n",
    "        subset = subset.append({\n",
    "            'datetime': datetime.datetime(year, 1 , 1),\n",
    "            'location': loc,\n",
    "            'count': np.nan\n",
    "        }, ignore_index=True)\n",
    "        subset = subset.append({\n",
    "            'datetime': datetime.datetime(year, 12 , 31),\n",
    "            'location': loc,\n",
    "            'count': np.nan\n",
    "        }, ignore_index=True)\n",
    "    subset.sort_values('datetime', inplace=True)\n",
    "    subset_resample = subset.groupby('location').resample(f'W-{get_dow(datetime.datetime(year, 1, 1))}',\n",
    "                                                          on='datetime', label='left').mean().interpolate(methods='linear')\n",
    "    year_juv_list.append(subset_resample)\n",
    "X_wild_juv = pd.concat(year_juv_list).reset_index().set_index('datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Temperature\n",
    "site_data = site_data[site_data['location'] == 'Glacier']\n",
    "\n",
    "site_data['datetime'] = pd.to_datetime(site_data[['year', 'month', 'day']])\n",
    "\n",
    "year_temp_list = []\n",
    "for year in analysis_years:\n",
    "    subset = site_data.loc[(site_data['datetime'].dt.year == year), ['datetime', 'temp', 'location']]\n",
    "    for loc in wild_locations:\n",
    "        subset = subset.append({\n",
    "            'datetime': datetime.datetime(year, 1 , 1),\n",
    "            'location': loc,\n",
    "            'temp': np.nan\n",
    "        }, ignore_index=True)\n",
    "        subset = subset.append({\n",
    "            'datetime': datetime.datetime(year, 12 , 31),\n",
    "            'location': loc,\n",
    "            'temp': np.nan\n",
    "        }, ignore_index=True)\n",
    "    subset.sort_values('datetime', inplace=True)\n",
    "    subset.sort_values('datetime', inplace=True)\n",
    "    subset_resample = subset.groupby('location').resample(f'W-{get_dow(datetime.datetime(year, 1, 1))}',\n",
    "                                                          on='datetime', label='left').mean().interpolate(methods='linear')\n",
    "    year_temp_list.append(subset_resample)\n",
    "X_wild_temp = pd.concat(year_temp_list).reset_index().set_index('datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Salinity\n",
    "year_sal_list = []\n",
    "for year in analysis_years:\n",
    "    subset = site_data.loc[(site_data['datetime'].dt.year == year), ['datetime', 'salt', 'location']]\n",
    "    for loc in wild_locations:\n",
    "        subset = subset.append({\n",
    "            'datetime': datetime.datetime(year, 1 , 1),\n",
    "            'location': loc,\n",
    "            'salt': np.nan\n",
    "        }, ignore_index=True)\n",
    "        subset = subset.append({\n",
    "            'datetime': datetime.datetime(year, 12 , 31),\n",
    "            'location': loc,\n",
    "            'salt': np.nan\n",
    "        }, ignore_index=True)\n",
    "    subset.sort_values('datetime', inplace=True)\n",
    "    subset_resample = subset.groupby('location').resample(f'W-{get_dow(datetime.datetime(year, 1, 1))}',\n",
    "                                                          on='datetime', label='left').mean().interpolate(method='linear')\n",
    "    year_sal_list.append(subset_resample)\n",
    "X_wild_sal = pd.concat(year_sal_list).reset_index().set_index('datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/EEB498/lib/python3.6/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n",
      "/opt/anaconda3/envs/EEB498/lib/python3.6/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/opt/anaconda3/envs/EEB498/lib/python3.6/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "## Farm data\n",
    "relevant_farms_iterable = ['Sargeaunt Pass',\n",
    "                           'Doctor Islets',\n",
    "                           'Humphrey Rock',\n",
    "                           'Burdwood',\n",
    "                           'Glacier Falls',\n",
    "                           'Sir Edmund Bay',\n",
    "                           'Wicklow Point'\n",
    "                          ]\n",
    "\n",
    "relevant_farm_data = industry_data[industry_data['Site Common Name'].str.contains('|'.join(relevant_farms_iterable))]\n",
    "\n",
    "relevant_farm_data['Day'] = 1\n",
    "relevant_farm_data['month'] = relevant_farm_data['Month'].map(month_map)\n",
    "relevant_farm_data['datetime'] = pd.to_datetime(relevant_farm_data[['Year', 'month', 'Day']])\n",
    "\n",
    "relevant_farm_data = relevant_farm_data[relevant_farm_data['datetime'].dt.year.isin(analysis_years)]\n",
    "\n",
    "year_industry_list = []\n",
    "for year in analysis_years:\n",
    "    subset = relevant_farm_data.loc[(relevant_farm_data['datetime'].dt.year == year), \n",
    "                                   ['datetime', 'Site Common Name', 'Average L. salmonis motiles per fish']]\n",
    "    \n",
    "    for i, farm in enumerate(relevant_farms_iterable):\n",
    "        subset = subset.append({\n",
    "            'datetime': datetime.datetime(year, 1 , 1),\n",
    "            'Site Common Name': farm,\n",
    "            'Average L. salmonis motiles per fish': np.nan\n",
    "        }, ignore_index=True)\n",
    "        subset = subset.append({\n",
    "            'datetime': datetime.datetime(year, 12 , 31),\n",
    "            'Site Common Name': farm,\n",
    "            'Average L. salmonis motiles per fish': np.nan\n",
    "        }, ignore_index=True)\n",
    "            \n",
    "    subset.sort_values('datetime', inplace=True)\n",
    "    subset_resample = subset.groupby('Site Common Name').resample(f'W-{get_dow(datetime.datetime(year, 1, 1))}',\n",
    "                                                                   on='datetime', label='left').mean().interpolate(methods='linear')\n",
    "\n",
    "    year_industry_list.append(subset_resample)\n",
    "X_industry = pd.DataFrame(pd.concat(year_industry_list).reset_index().groupby('datetime')['Average L. salmonis motiles per fish'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Weather data\n",
    "station_dir = '../station_data/'\n",
    "\n",
    "station_files = os.listdir(station_dir)\n",
    "station_files = [file for file in station_files if 'en_climate_daily_BC' in file]\n",
    "\n",
    "stations_to_concat = []\n",
    "for file in station_files:\n",
    "    stations_to_concat.append(pd.read_csv(f'{station_dir}{file}'))\n",
    "station_df = pd.concat(stations_to_concat)\n",
    "station_df['datetime'] = pd.to_datetime(station_df['Date/Time'])\n",
    "\n",
    "relevant_station_df = station_df[station_df['datetime'].dt.year.isin(analysis_years)]\n",
    "year_station_list = []\n",
    "\n",
    "for year in analysis_years:\n",
    "    subset = relevant_station_df.loc[(relevant_station_df['datetime'].dt.year == year),\n",
    "                                    ['datetime', 'Mean Temp (°C)']].rename({'Mean Temp (°C)': 'temp'})\n",
    "    subset.sort_values('datetime', inplace=True)\n",
    "    subset_resample = subset.resample(f'W-{get_dow(datetime.datetime(year, 1, 1))}',\n",
    "                                      on='datetime', label='left').apply(np.nanmean).interpolate(methods='linear')\n",
    "    year_station_list.append(subset_resample)\n",
    "X_station = pd.concat(year_station_list).reset_index().set_index('datetime')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unified X\n",
    "X_values = pd.concat([X_industry, X_wild_juv, X_wild_sal, X_wild_temp, X_station], axis=1).drop('location', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name_dict = {'Average L. salmonis motiles per fish': 'industry', 'count': 'nonmotiles', 'salt': 'salt', 'temp': 'temp', 'Mean Temp (°C)': 'station_temp'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_values = X_values.rename(column_name_dict, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Overall dataset\n",
    "data = pd.concat([X_values, Y_glacier], axis=1).rename({'count': 'motiles'}, axis=1).reset_index()\n",
    "data['year'] = data['datetime'].dt.year\n",
    "data = data[data['year'] != 2002] # remove dummy year\n",
    "dummy_date_filter = (data['datetime'].dt.month == 12) & (data['datetime'].dt.day == 25) # have to filter out dummy dates\n",
    "leap_date_filter = (data['datetime'].dt.month == 12) & (data['datetime'].dt.day == 30) # have to filter out artificial extra week from leap year\n",
    "data = data[~(dummy_date_filter | leap_date_filter) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create dummy dates as year is just the sample\n",
    "dummy_year_dates = data[data['year'] == 2003]['datetime']\n",
    "data_test = data.copy()\n",
    "\n",
    "data_test['datetime'] = pd.concat([dummy_year_dates] * data_test['year'].nunique(), axis=0).values\n",
    "# data_test['week_of_year'] = list(range(1, 53)) * data_test['year'].nunique()\n",
    "data_test['week_of_year'] = data_test['datetime'].dt.week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test different data input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_data_g = fish_data[fish_data['location'] == 'Glacier']\n",
    "\n",
    "nonmotile_fish_data = fish_data_g[['Lep_cope', 'chalA', 'chalB', 'Caligus_cope', 'unid_cope', 'chal_unid']].sum(axis=1)\n",
    "motile_fish_data = fish_data_g[['Lep_PAmale', 'Lep_PAfemale', 'Lep_male', 'Lep_gravid', 'Lep_nongravid', 'unid_PA', 'unid_adult']].sum(axis=1)\n",
    "fish_data_dates = pd.to_datetime(fish_data_g[['day', 'month', 'year']])\n",
    "\n",
    "\n",
    "fish_data_inputs = pd.concat([fish_data_dates, nonmotile_fish_data, motile_fish_data], axis=1).rename({0:'datetime', 1:'nonmotiles', 2:'motiles'}, axis=1).set_index('datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_data_g = site_data[site_data['location'] == 'Glacier']\n",
    "\n",
    "site_data_dates = pd.to_datetime(site_data_g[['day', 'month', 'year']])\n",
    "site_data_temp = site_data_g['temp']\n",
    "site_data_salt = site_data_g['salt']\n",
    "\n",
    "\n",
    "site_data_inputs = pd.concat([site_data_dates, site_data_temp, site_data_salt], axis=1).rename({0: 'datetime'}, axis=1).set_index('datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "relevant_farm_data = industry_data[industry_data['Site Common Name'].str.contains('|'.join(relevant_farms_iterable))]\n",
    "\n",
    "relevant_farm_data['Day'] = 1\n",
    "month_map = {\n",
    "    'January': 1,\n",
    "    'February': 2,\n",
    "    'March': 3,\n",
    "    'April': 4,\n",
    "    'May': 5,\n",
    "    'June': 6,\n",
    "    'July': 7,\n",
    "    'August': 8,\n",
    "    'September': 9,\n",
    "    'October': 10,\n",
    "    'November': 11,\n",
    "    'December': 12\n",
    "}\n",
    "relevant_farm_data['month'] = relevant_farm_data['Month'].map(month_map)\n",
    "relevant_farm_data['datetime'] = pd.to_datetime(relevant_farm_data[['Year', 'month', 'Day']])\n",
    "\n",
    "farm_data_inputs = relevant_farm_data[['datetime', 'Site Common Name', 'Average L. salmonis motiles per fish']].groupby('datetime').mean()\n",
    "farm_data_inputs = farm_data_inputs.rename({'Average L. salmonis motiles per fish': 'agg_farm_motiles'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_dir = 'weather_data/'\n",
    "\n",
    "station_files = os.listdir(station_dir)\n",
    "station_files = [file for file in station_files if 'en_climate_daily_BC' in file]\n",
    "\n",
    "stations_to_concat = []\n",
    "for file in station_files:\n",
    "    stations_to_concat.append(pd.read_csv(f'{station_dir}{file}'))\n",
    "station_df = pd.concat(stations_to_concat)\n",
    "station_df['datetime'] = pd.to_datetime(station_df['Date/Time'])\n",
    "\n",
    "relevant_station_df = station_df[station_df['datetime'].dt.year.isin(analysis_years)]\n",
    "weather_station_inputs = relevant_station_df[['datetime', 'Mean Temp (°C)']].rename({'Mean Temp (°C)': 'station_temp'}, axis=1).set_index('datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_data = weather_station_inputs.merge(fish_data_inputs, left_index=True, right_index=True, how='left')\n",
    "all_input_data = all_input_data.merge(site_data_inputs, left_index=True, right_index=True, how='left')\n",
    "all_input_data = all_input_data.merge(farm_data_inputs, left_index=True, right_index=True, how='left')\n",
    "\n",
    "all_input_data = all_input_data[all_input_data.index.year.isin(analysis_years)]\n",
    "input_data = all_input_data.resample('W').mean().reset_index()\n",
    "# input_data['datetime'] = pd.to_datetime(input_data['datetime'])\n",
    "# input_data = input_data.rename({'datetime': 'week_datetime'}, axis=1)\n",
    "input_data['week_of_year'] = input_data['datetime'].dt.week\n",
    "# input_data['year'] = input_data['year'].astype(int)\n",
    "# input_data['week_of_year'] = input_data['week_of_year'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = input_data.loc[:, input_data.columns != 'year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup AutomL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y column\n",
    "label = 'motiles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = data_test[data_test['datetime'].dt.month <= 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "locally_stored_runs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local machine\n",
      "Parent Run ID: AutoML_6a3e2a0e-6140-4a37-b53b-759db2793b2e\n",
      "\n",
      "Current status: DatasetFeaturization. Beginning to featurize the dataset.\n",
      "Current status: DatasetEvaluation. Gathering dataset statistics.\n",
      "Current status: FeaturesGeneration. Generating features for the dataset.\n",
      "Current status: DatasetFeaturizationCompleted. Completed featurizing the dataset.\n",
      "Current status: DatasetCrossValidationSplit. Generating individually featurized CV splits.\n",
      "\n",
      "****************************************************************************************************\n",
      "DATA GUARDRAILS: \n",
      "\n",
      "TYPE:         Missing values imputation\n",
      "STATUS:       FIXED\n",
      "DESCRIPTION:  The training data had the following missing values which were resolved. Please review your data source for data quality issues and possibly filter out the rows with these missing values. If the missing values are expected, you can either accept the above imputation, or implement your own custom imputation that may be more appropriate based on the data type and business process.\n",
      "PARAMETERS:   Column name : industry, Imputation type : mean\n",
      "              Column name : temp, Imputation type : mean\n",
      "              \n",
      "TYPE:         High cardinality feature detection\n",
      "STATUS:       PASSED\n",
      "DESCRIPTION:  Your inputs were analyzed, and no high cardinality features were detected.\n",
      "\n",
      "****************************************************************************************************\n",
      "Current status: ModelSelection. Beginning model selection.\n",
      "\n",
      "****************************************************************************************************\n",
      "ITERATION: The iteration being evaluated.\n",
      "PIPELINE: A summary description of the pipeline being evaluated.\n",
      "DURATION: Time taken for the current iteration.\n",
      "METRIC: The result of computing score on the fitted pipeline.\n",
      "BEST: The best observed score thus far.\n",
      "****************************************************************************************************\n",
      "\n",
      " ITERATION   PIPELINE                                       DURATION      METRIC      BEST\n",
      "         0   StandardScalerWrapper ElasticNet               0:00:15       0.0456    0.0456\n",
      "         1   StandardScalerWrapper ElasticNet               0:00:15       0.0458    0.0456\n",
      "         2   StandardScalerWrapper ElasticNet               0:00:15       0.0474    0.0456\n",
      "         3   StandardScalerWrapper ElasticNet               0:00:16       0.0720    0.0456\n",
      "         4   StandardScalerWrapper DecisionTree             0:00:15       0.0143    0.0143\n",
      "         5   MaxAbsScaler GradientBoosting                  0:00:15       0.0651    0.0143\n",
      "         6   MaxAbsScaler DecisionTree                      0:00:15       0.0138    0.0138\n",
      "         7   StandardScalerWrapper ElasticNet               0:00:21       0.0919    0.0138\n",
      "         8   MaxAbsScaler RandomForest                      0:00:23       0.0135    0.0135\n",
      "         9   MaxAbsScaler DecisionTree                      0:00:33       0.0141    0.0135\n",
      "        10   StandardScalerWrapper ElasticNet               0:00:14       0.0479    0.0135\n",
      "        11   StandardScalerWrapper ElasticNet               0:00:16       0.0458    0.0135\n",
      "        12   StandardScalerWrapper ElasticNet               0:00:14       0.0459    0.0135\n",
      "        13   MaxAbsScaler DecisionTree                      0:00:16       0.0162    0.0135\n",
      "        14   MaxAbsScaler ElasticNet                        0:00:17       0.0991    0.0135\n",
      "        15   StandardScalerWrapper ElasticNet               0:00:14       0.0469    0.0135\n",
      "        16   StandardScalerWrapper ElasticNet               0:00:18       0.0504    0.0135\n",
      "        17   StandardScalerWrapper DecisionTree             0:00:14       0.0122    0.0122\n",
      "        18   MaxAbsScaler DecisionTree                      0:00:16       0.0198    0.0122\n",
      "        19   MaxAbsScaler DecisionTree                      0:00:15       0.0142    0.0122\n",
      "        20   StandardScalerWrapper ElasticNet               0:00:16       0.0460    0.0122\n",
      "        21   StandardScalerWrapper ElasticNet               0:00:23       0.0460    0.0122\n",
      "        22   TruncatedSVDWrapper ElasticNet                 0:00:23       0.0519    0.0122\n",
      "        23   StandardScalerWrapper ElasticNet               0:00:23       0.0728    0.0122\n",
      "        24   StandardScalerWrapper ElasticNet               0:00:17       0.0455    0.0122\n",
      "        25   MaxAbsScaler DecisionTree                      0:00:16       0.0356    0.0122\n",
      "        26   TruncatedSVDWrapper ElasticNet                 0:00:16       0.0463    0.0122\n",
      "        27   StandardScalerWrapper ElasticNet               0:00:15       0.0467    0.0122\n",
      "        28   StandardScalerWrapper ElasticNet               0:00:16       0.0482    0.0122\n",
      "        29   MaxAbsScaler ElasticNet                        0:00:15       0.0606    0.0122\n",
      "        30   VotingEnsemble                                 0:00:23       0.0100    0.0100\n",
      "        31   StackEnsemble                                  0:00:29       0.0125    0.0100\n",
      "Stopping criteria reached at iteration 31. Ending experiment.\n",
      "Running on local machine\n",
      "Parent Run ID: AutoML_3f595a7f-02cd-41fa-af11-1b935dc4f4db\n",
      "\n",
      "Current status: DatasetFeaturization. Beginning to featurize the dataset.\n",
      "Current status: DatasetEvaluation. Gathering dataset statistics.\n",
      "Current status: FeaturesGeneration. Generating features for the dataset.\n",
      "Current status: DatasetFeaturizationCompleted. Completed featurizing the dataset.\n",
      "Current status: DatasetCrossValidationSplit. Generating individually featurized CV splits.\n",
      "\n",
      "****************************************************************************************************\n",
      "DATA GUARDRAILS: \n",
      "\n",
      "TYPE:         Missing values imputation\n",
      "STATUS:       FIXED\n",
      "DESCRIPTION:  The training data had the following missing values which were resolved. Please review your data source for data quality issues and possibly filter out the rows with these missing values. If the missing values are expected, you can either accept the above imputation, or implement your own custom imputation that may be more appropriate based on the data type and business process.\n",
      "PARAMETERS:   Column name : industry, Imputation type : mean\n",
      "              Column name : temp, Imputation type : mean\n",
      "              \n",
      "TYPE:         High cardinality feature detection\n",
      "STATUS:       PASSED\n",
      "DESCRIPTION:  Your inputs were analyzed, and no high cardinality features were detected.\n",
      "\n",
      "****************************************************************************************************\n",
      "Current status: ModelSelection. Beginning model selection.\n",
      "\n",
      "****************************************************************************************************\n",
      "ITERATION: The iteration being evaluated.\n",
      "PIPELINE: A summary description of the pipeline being evaluated.\n",
      "DURATION: Time taken for the current iteration.\n",
      "METRIC: The result of computing score on the fitted pipeline.\n",
      "BEST: The best observed score thus far.\n",
      "****************************************************************************************************\n",
      "\n",
      " ITERATION   PIPELINE                                       DURATION      METRIC      BEST\n",
      "         0   StandardScalerWrapper ElasticNet               0:00:14       0.1185    0.1185\n",
      "         1   StandardScalerWrapper ElasticNet               0:00:16       0.1197    0.1185\n",
      "         2   StandardScalerWrapper ElasticNet               0:00:16       0.1334    0.1185\n",
      "         3   StandardScalerWrapper ElasticNet               0:00:17       0.1385    0.1185\n",
      "         4   StandardScalerWrapper DecisionTree             0:00:14       0.0351    0.0351\n",
      "         5   MaxAbsScaler GradientBoosting                  0:00:16       0.0886    0.0351\n",
      "         6   MaxAbsScaler DecisionTree                      0:00:14       0.0371    0.0351\n",
      "         7   StandardScalerWrapper ElasticNet               0:00:14       0.1385    0.0351\n",
      "         8   MaxAbsScaler RandomForest                      0:00:15       0.0334    0.0334\n",
      "         9   MaxAbsScaler DecisionTree                      0:00:14       0.0284    0.0284\n",
      "        10   StandardScalerWrapper ElasticNet               0:00:15       0.1378    0.0284\n",
      "        11   StandardScalerWrapper ElasticNet               0:00:14       0.1197    0.0284\n",
      "        12   StandardScalerWrapper ElasticNet               0:00:15       0.1246    0.0284\n",
      "        13   MaxAbsScaler DecisionTree                      0:00:15       0.0355    0.0284\n",
      "        14   MaxAbsScaler ElasticNet                        0:00:15       0.1385    0.0284\n",
      "        15   StandardScalerWrapper ElasticNet               0:00:16       0.1201    0.0284\n",
      "        16   StandardScalerWrapper ElasticNet               0:00:14       0.1209    0.0284\n",
      "        17   StandardScalerWrapper DecisionTree             0:00:14       0.0332    0.0284\n",
      "        18   MaxAbsScaler DecisionTree                      0:00:14       0.0498    0.0284\n",
      "        19   MaxAbsScaler DecisionTree                      0:00:14       0.0373    0.0284\n",
      "        20   StandardScalerWrapper ElasticNet               0:00:17       0.1197    0.0284\n",
      "        21   StandardScalerWrapper ElasticNet               0:00:15       0.1197    0.0284\n",
      "        22   TruncatedSVDWrapper ElasticNet                 0:00:15       0.1198    0.0284\n",
      "        23   StandardScalerWrapper ElasticNet               0:00:16       0.1385    0.0284\n",
      "        24   MaxAbsScaler DecisionTree                      0:00:17       0.0759    0.0284\n",
      "        25   MaxAbsScaler DecisionTree                      0:00:17       0.0481    0.0284\n",
      "        26   StandardScalerWrapper ElasticNet               0:00:16       0.1161    0.0284\n",
      "        27   TruncatedSVDWrapper ElasticNet                 0:00:15       0.1141    0.0284\n",
      "        28   MaxAbsScaler ElasticNet                        0:00:15       0.1383    0.0284\n",
      "        29   StandardScalerWrapper ElasticNet               0:00:15       0.1192    0.0284\n",
      "        30   VotingEnsemble                                 0:00:26       0.0258    0.0258\n",
      "        31   StackEnsemble                                  0:00:25       0.0303    0.0258\n",
      "Stopping criteria reached at iteration 31. Ending experiment.\n",
      "Running on local machine\n",
      "Parent Run ID: AutoML_9dfa9e96-e0b2-487a-9955-14036d1c5b82\n",
      "\n",
      "Current status: DatasetFeaturization. Beginning to featurize the dataset.\n",
      "Current status: DatasetEvaluation. Gathering dataset statistics.\n",
      "Current status: FeaturesGeneration. Generating features for the dataset.\n",
      "Current status: DatasetFeaturizationCompleted. Completed featurizing the dataset.\n",
      "Current status: DatasetCrossValidationSplit. Generating individually featurized CV splits.\n",
      "\n",
      "****************************************************************************************************\n",
      "DATA GUARDRAILS: \n",
      "\n",
      "TYPE:         Missing values imputation\n",
      "STATUS:       FIXED\n",
      "DESCRIPTION:  The training data had the following missing values which were resolved. Please review your data source for data quality issues and possibly filter out the rows with these missing values. If the missing values are expected, you can either accept the above imputation, or implement your own custom imputation that may be more appropriate based on the data type and business process.\n",
      "PARAMETERS:   Column name : industry, Imputation type : mean\n",
      "              Column name : temp, Imputation type : mean\n",
      "              \n",
      "TYPE:         High cardinality feature detection\n",
      "STATUS:       PASSED\n",
      "DESCRIPTION:  Your inputs were analyzed, and no high cardinality features were detected.\n",
      "\n",
      "****************************************************************************************************\n",
      "Current status: ModelSelection. Beginning model selection.\n",
      "\n",
      "****************************************************************************************************\n",
      "ITERATION: The iteration being evaluated.\n",
      "PIPELINE: A summary description of the pipeline being evaluated.\n",
      "DURATION: Time taken for the current iteration.\n",
      "METRIC: The result of computing score on the fitted pipeline.\n",
      "BEST: The best observed score thus far.\n",
      "****************************************************************************************************\n",
      "\n",
      " ITERATION   PIPELINE                                       DURATION      METRIC      BEST\n",
      "         0   StandardScalerWrapper ElasticNet               0:00:14       0.0440    0.0440\n",
      "         1   StandardScalerWrapper ElasticNet               0:00:14       0.0439    0.0439\n",
      "         2   StandardScalerWrapper ElasticNet               0:00:14       0.0465    0.0439\n",
      "         3   StandardScalerWrapper ElasticNet               0:00:15       0.0707    0.0439\n",
      "         4   StandardScalerWrapper DecisionTree             0:00:15       0.0158    0.0158\n",
      "         5   MaxAbsScaler GradientBoosting                  0:00:15       0.0630    0.0158\n",
      "         6   MaxAbsScaler DecisionTree                      0:00:15       0.0137    0.0137\n",
      "         7   StandardScalerWrapper ElasticNet               0:00:14       0.0884    0.0137\n",
      "         8   MaxAbsScaler RandomForest                      0:00:15       0.0136    0.0136\n",
      "         9   MaxAbsScaler DecisionTree                      0:00:15       0.0152    0.0136\n",
      "        10   StandardScalerWrapper ElasticNet               0:00:15       0.0469    0.0136\n",
      "        11   StandardScalerWrapper ElasticNet               0:00:14       0.0440    0.0136\n",
      "        12   StandardScalerWrapper ElasticNet               0:00:15       0.0444    0.0136\n",
      "        13   MaxAbsScaler DecisionTree                      0:00:17       0.0123    0.0123\n",
      "        14   MaxAbsScaler ElasticNet                        0:00:13       0.0965    0.0123\n",
      "        15   StandardScalerWrapper ElasticNet               0:00:20       0.0447    0.0123\n",
      "        16   StandardScalerWrapper ElasticNet               0:00:22       0.0479    0.0123\n",
      "        17   StandardScalerWrapper DecisionTree             0:00:22       0.0141    0.0123\n",
      "        18   MaxAbsScaler DecisionTree                      0:00:18       0.0151    0.0123\n",
      "        19   MaxAbsScaler DecisionTree                      0:00:15       0.0111    0.0111\n",
      "        20   StandardScalerWrapper ElasticNet               0:00:16       0.0441    0.0111\n",
      "        21   StandardScalerWrapper ElasticNet               0:00:16       0.0441    0.0111\n",
      "        22   TruncatedSVDWrapper ElasticNet                 0:00:16       0.0491    0.0111\n",
      "        23   StandardScalerWrapper ElasticNet               0:00:15       0.0442    0.0111\n",
      "        24   StandardScalerWrapper ElasticNet               0:00:17       0.0700    0.0111\n",
      "        25   MaxAbsScaler DecisionTree                      0:00:17       0.0400    0.0111\n",
      "        26   TruncatedSVDWrapper ElasticNet                 0:00:17       0.0448    0.0111\n",
      "        27   StandardScalerWrapper ElasticNet               0:00:15       0.0447    0.0111\n",
      "        28   MaxAbsScaler ElasticNet                        0:00:15       0.0598    0.0111\n",
      "        29   StandardScalerWrapper ElasticNet               0:00:15       0.0462    0.0111\n",
      "        30   VotingEnsemble                                 0:00:25       0.0087    0.0087\n",
      "        31   StackEnsemble                                  0:00:23       0.0113    0.0087\n",
      "Stopping criteria reached at iteration 31. Ending experiment.\n",
      "Running on local machine\n",
      "Parent Run ID: AutoML_80e08b3d-28ca-4b5e-b973-c8154a3feac3\n",
      "\n",
      "Current status: DatasetFeaturization. Beginning to featurize the dataset.\n",
      "Current status: DatasetEvaluation. Gathering dataset statistics.\n",
      "Current status: FeaturesGeneration. Generating features for the dataset.\n",
      "Current status: DatasetFeaturizationCompleted. Completed featurizing the dataset.\n",
      "Current status: DatasetCrossValidationSplit. Generating individually featurized CV splits.\n",
      "\n",
      "****************************************************************************************************\n",
      "DATA GUARDRAILS: \n",
      "\n",
      "TYPE:         Missing values imputation\n",
      "STATUS:       FIXED\n",
      "DESCRIPTION:  The training data had the following missing values which were resolved. Please review your data source for data quality issues and possibly filter out the rows with these missing values. If the missing values are expected, you can either accept the above imputation, or implement your own custom imputation that may be more appropriate based on the data type and business process.\n",
      "PARAMETERS:   Column name : industry, Imputation type : mean\n",
      "              Column name : temp, Imputation type : mean\n",
      "              \n",
      "TYPE:         High cardinality feature detection\n",
      "STATUS:       PASSED\n",
      "DESCRIPTION:  Your inputs were analyzed, and no high cardinality features were detected.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****************************************************************************************************\n",
      "Current status: ModelSelection. Beginning model selection.\n",
      "\n",
      "****************************************************************************************************\n",
      "ITERATION: The iteration being evaluated.\n",
      "PIPELINE: A summary description of the pipeline being evaluated.\n",
      "DURATION: Time taken for the current iteration.\n",
      "METRIC: The result of computing score on the fitted pipeline.\n",
      "BEST: The best observed score thus far.\n",
      "****************************************************************************************************\n",
      "\n",
      " ITERATION   PIPELINE                                       DURATION      METRIC      BEST\n",
      "         0   StandardScalerWrapper ElasticNet               0:00:15       0.0471    0.0471\n",
      "         1   StandardScalerWrapper ElasticNet               0:00:14       0.0470    0.0470\n",
      "         2   StandardScalerWrapper ElasticNet               0:00:14       0.0492    0.0470\n",
      "         3   StandardScalerWrapper ElasticNet               0:00:14       0.0747    0.0470\n",
      "         4   StandardScalerWrapper DecisionTree             0:00:14       0.0141    0.0141\n",
      "         5   MaxAbsScaler GradientBoosting                  0:00:16       0.0663    0.0141\n",
      "         6   MaxAbsScaler DecisionTree                      0:00:14       0.0164    0.0141\n",
      "         7   StandardScalerWrapper ElasticNet               0:00:16       0.0924    0.0141\n",
      "         8   MaxAbsScaler RandomForest                      0:00:15       0.0137    0.0137\n",
      "         9   MaxAbsScaler DecisionTree                      0:00:17       0.0169    0.0137\n",
      "        10   StandardScalerWrapper ElasticNet               0:00:15       0.0491    0.0137\n",
      "        11   StandardScalerWrapper ElasticNet               0:00:14       0.0470    0.0137\n",
      "        12   StandardScalerWrapper ElasticNet               0:00:16       0.0474    0.0137\n",
      "        13   MaxAbsScaler DecisionTree                      0:00:14       0.0188    0.0137\n",
      "        14   MaxAbsScaler ElasticNet                        0:00:14       0.0997    0.0137\n",
      "        15   StandardScalerWrapper ElasticNet               0:00:14       0.0477    0.0137\n",
      "        16   StandardScalerWrapper ElasticNet               0:00:14       0.0509    0.0137\n",
      "        17   StandardScalerWrapper DecisionTree             0:00:13       0.0187    0.0137\n",
      "        18   MaxAbsScaler DecisionTree                      0:00:15       0.0187    0.0137\n",
      "        19   MaxAbsScaler DecisionTree                      0:00:14       0.0110    0.0110\n",
      "        20   StandardScalerWrapper ElasticNet               0:00:15       0.0472    0.0110\n",
      "        21   StandardScalerWrapper ElasticNet               0:00:17       0.0472    0.0110\n",
      "        22   TruncatedSVDWrapper ElasticNet                 0:00:15       0.0523    0.0110\n",
      "        23   StandardScalerWrapper ElasticNet               0:00:15       0.0740    0.0110\n",
      "        24   StandardScalerWrapper ElasticNet               0:00:14       0.0471    0.0110\n",
      "        25   MaxAbsScaler DecisionTree                      0:00:16       0.0351    0.0110\n",
      "        26   TruncatedSVDWrapper ElasticNet                 0:00:15       0.0478    0.0110\n",
      "        27   StandardScalerWrapper ElasticNet               0:00:15       0.0479    0.0110\n",
      "        28   StandardScalerWrapper ElasticNet               0:00:16       0.0492    0.0110\n",
      "        29   MaxAbsScaler ElasticNet                        0:00:15       0.0626    0.0110\n",
      "        30   VotingEnsemble                                 0:00:26       0.0100    0.0100\n",
      "        31   StackEnsemble                                  0:00:25       0.0130    0.0100\n",
      "Stopping criteria reached at iteration 31. Ending experiment.\n",
      "Running on local machine\n",
      "Parent Run ID: AutoML_1f16cf1c-9a71-480f-8f24-90a83cdf7e1c\n",
      "\n",
      "Current status: DatasetFeaturization. Beginning to featurize the dataset.\n",
      "Current status: DatasetEvaluation. Gathering dataset statistics.\n",
      "Current status: FeaturesGeneration. Generating features for the dataset.\n",
      "Current status: DatasetFeaturizationCompleted. Completed featurizing the dataset.\n",
      "Current status: DatasetCrossValidationSplit. Generating individually featurized CV splits.\n",
      "\n",
      "****************************************************************************************************\n",
      "DATA GUARDRAILS: \n",
      "\n",
      "TYPE:         Missing values imputation\n",
      "STATUS:       FIXED\n",
      "DESCRIPTION:  The training data had the following missing values which were resolved. Please review your data source for data quality issues and possibly filter out the rows with these missing values. If the missing values are expected, you can either accept the above imputation, or implement your own custom imputation that may be more appropriate based on the data type and business process.\n",
      "PARAMETERS:   Column name : industry, Imputation type : mean\n",
      "              Column name : temp, Imputation type : mean\n",
      "              \n",
      "TYPE:         High cardinality feature detection\n",
      "STATUS:       PASSED\n",
      "DESCRIPTION:  Your inputs were analyzed, and no high cardinality features were detected.\n",
      "\n",
      "****************************************************************************************************\n",
      "Current status: ModelSelection. Beginning model selection.\n",
      "\n",
      "****************************************************************************************************\n",
      "ITERATION: The iteration being evaluated.\n",
      "PIPELINE: A summary description of the pipeline being evaluated.\n",
      "DURATION: Time taken for the current iteration.\n",
      "METRIC: The result of computing score on the fitted pipeline.\n",
      "BEST: The best observed score thus far.\n",
      "****************************************************************************************************\n",
      "\n",
      " ITERATION   PIPELINE                                       DURATION      METRIC      BEST\n",
      "         0   StandardScalerWrapper ElasticNet               0:00:15       0.0463    0.0463\n",
      "         1   StandardScalerWrapper ElasticNet               0:00:15       0.0466    0.0463\n",
      "         2   StandardScalerWrapper ElasticNet               0:00:14       0.0489    0.0463\n",
      "         3   StandardScalerWrapper ElasticNet               0:00:14       0.0741    0.0463\n",
      "         4   StandardScalerWrapper DecisionTree             0:00:14       0.0133    0.0133\n",
      "         5   MaxAbsScaler GradientBoosting                  0:00:18       0.0646    0.0133\n",
      "         6   MaxAbsScaler DecisionTree                      0:00:15       0.0137    0.0133\n",
      "         7   StandardScalerWrapper ElasticNet               0:00:14       0.0925    0.0133\n",
      "         8   MaxAbsScaler RandomForest                      0:00:13       0.0133    0.0133\n",
      "         9   MaxAbsScaler DecisionTree                      0:00:14       0.0163    0.0133\n",
      "        10   StandardScalerWrapper ElasticNet               0:00:24       0.0495    0.0133\n",
      "        11   StandardScalerWrapper ElasticNet               0:00:22       0.0467    0.0133\n",
      "        12   StandardScalerWrapper ElasticNet               0:00:22       0.0471    0.0133\n",
      "        13   MaxAbsScaler DecisionTree                      0:00:17       0.0168    0.0133\n",
      "        14   MaxAbsScaler ElasticNet                        0:00:14       0.0989    0.0133\n",
      "        15   StandardScalerWrapper ElasticNet               0:00:16       0.0478    0.0133\n",
      "        16   StandardScalerWrapper ElasticNet               0:00:16       0.0508    0.0133\n",
      "        17   StandardScalerWrapper DecisionTree             0:00:16       0.0124    0.0124\n",
      "        18   MaxAbsScaler DecisionTree                      0:00:14       0.0190    0.0124\n",
      "        19   MaxAbsScaler DecisionTree                      0:00:15       0.0137    0.0124\n",
      "        20   StandardScalerWrapper ElasticNet               0:00:16       0.0467    0.0124\n",
      "        21   StandardScalerWrapper ElasticNet               0:00:16       0.0467    0.0124\n",
      "        22   TruncatedSVDWrapper ElasticNet                 0:00:17       0.0522    0.0124\n",
      "        23   StandardScalerWrapper ElasticNet               0:00:18       0.0745    0.0124\n",
      "        24   StandardScalerWrapper ElasticNet               0:00:19       0.0459    0.0124\n",
      "        25   MaxAbsScaler DecisionTree                      0:00:19       0.0373    0.0124\n",
      "        26   TruncatedSVDWrapper ElasticNet                 0:00:15       0.0465    0.0124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        27   StandardScalerWrapper ElasticNet               0:00:17       0.0472    0.0124\n",
      "        28   StandardScalerWrapper ElasticNet               0:00:19       0.0499    0.0124\n",
      "        29   MaxAbsScaler ElasticNet                        0:00:16       0.0614    0.0124\n",
      "        30   VotingEnsemble                                 0:00:26       0.0085    0.0085\n",
      "        31   StackEnsemble                                  0:00:55       0.0097    0.0085\n",
      "Stopping criteria reached at iteration 31. Ending experiment.\n",
      "Running on local machine\n",
      "Parent Run ID: AutoML_c147ab41-c143-4fa0-873e-74ebf9543940\n",
      "\n",
      "Current status: DatasetFeaturization. Beginning to featurize the dataset.\n",
      "Current status: DatasetEvaluation. Gathering dataset statistics.\n",
      "Current status: FeaturesGeneration. Generating features for the dataset.\n",
      "Current status: DatasetFeaturizationCompleted. Completed featurizing the dataset.\n",
      "Current status: DatasetCrossValidationSplit. Generating individually featurized CV splits.\n",
      "\n",
      "****************************************************************************************************\n",
      "DATA GUARDRAILS: \n",
      "\n",
      "TYPE:         Missing values imputation\n",
      "STATUS:       FIXED\n",
      "DESCRIPTION:  The training data had the following missing values which were resolved. Please review your data source for data quality issues and possibly filter out the rows with these missing values. If the missing values are expected, you can either accept the above imputation, or implement your own custom imputation that may be more appropriate based on the data type and business process.\n",
      "PARAMETERS:   Column name : industry, Imputation type : mean\n",
      "              Column name : temp, Imputation type : mean\n",
      "              \n",
      "TYPE:         High cardinality feature detection\n",
      "STATUS:       PASSED\n",
      "DESCRIPTION:  Your inputs were analyzed, and no high cardinality features were detected.\n",
      "\n",
      "****************************************************************************************************\n",
      "Current status: ModelSelection. Beginning model selection.\n",
      "\n",
      "****************************************************************************************************\n",
      "ITERATION: The iteration being evaluated.\n",
      "PIPELINE: A summary description of the pipeline being evaluated.\n",
      "DURATION: Time taken for the current iteration.\n",
      "METRIC: The result of computing score on the fitted pipeline.\n",
      "BEST: The best observed score thus far.\n",
      "****************************************************************************************************\n",
      "\n",
      " ITERATION   PIPELINE                                       DURATION      METRIC      BEST\n",
      "         0   StandardScalerWrapper ElasticNet               0:00:24       0.0452    0.0452\n",
      "         1   StandardScalerWrapper ElasticNet               0:00:22       0.0453    0.0452\n",
      "         2   StandardScalerWrapper ElasticNet               0:00:16       0.0464    0.0452\n",
      "         3   StandardScalerWrapper ElasticNet               0:00:16       0.0714    0.0452\n",
      "         4   StandardScalerWrapper DecisionTree             0:00:14       0.0120    0.0120\n",
      "         5   MaxAbsScaler GradientBoosting                  0:00:15       0.0642    0.0120\n",
      "         6   MaxAbsScaler DecisionTree                      0:00:14       0.0148    0.0120\n",
      "         7   StandardScalerWrapper ElasticNet               0:00:17       0.0904    0.0120\n",
      "         8   MaxAbsScaler RandomForest                      0:00:16       0.0113    0.0113\n",
      "         9   MaxAbsScaler DecisionTree                      0:00:16       0.0186    0.0113\n",
      "        10   StandardScalerWrapper ElasticNet               0:00:15       0.0467    0.0113\n",
      "        11   StandardScalerWrapper ElasticNet               0:00:16       0.0454    0.0113\n",
      "        12   StandardScalerWrapper ElasticNet               0:00:16       0.0446    0.0113\n",
      "        13   MaxAbsScaler DecisionTree                      0:00:17       0.0170    0.0113\n",
      "        14   MaxAbsScaler ElasticNet                        0:00:17       0.0975    0.0113\n",
      "        15   StandardScalerWrapper ElasticNet               0:00:18       0.0462    0.0113\n",
      "        16   StandardScalerWrapper ElasticNet               0:00:19       0.0495    0.0113\n",
      "        17   StandardScalerWrapper DecisionTree             0:00:17       0.0133    0.0113\n",
      "        18   MaxAbsScaler DecisionTree                      0:00:19       0.0136    0.0113\n",
      "        19   MaxAbsScaler DecisionTree                      0:00:18       0.0120    0.0113\n",
      "        20   StandardScalerWrapper ElasticNet               0:00:19       0.0455    0.0113\n",
      "        21   StandardScalerWrapper ElasticNet               0:00:15       0.0455    0.0113\n",
      "        22   TruncatedSVDWrapper ElasticNet                 0:00:19       0.0472    0.0113\n",
      "        23   StandardScalerWrapper ElasticNet               0:00:19       0.0718    0.0113\n",
      "        24   TruncatedSVDWrapper ElasticNet                 0:00:19       0.0461    0.0113\n",
      "        25   MaxAbsScaler RandomForest                      0:00:18       0.0241    0.0113\n",
      "        26   StandardScalerWrapper ElasticNet               0:00:18       0.0460    0.0113\n",
      "        27   StandardScalerWrapper ElasticNet               0:00:25       0.0471    0.0113\n",
      "        28   StandardScalerWrapper ElasticNet               0:00:25       0.0449    0.0113\n",
      "        29   MaxAbsScaler DecisionTree                      0:00:25       0.0320    0.0113\n",
      "        30   VotingEnsemble                                 0:00:30       0.0098    0.0098\n",
      "        31   StackEnsemble                                  0:00:35       0.0157    0.0098\n",
      "Stopping criteria reached at iteration 31. Ending experiment.\n",
      "Running on local machine\n",
      "Parent Run ID: AutoML_efc39ca7-c2ba-4c58-b2ac-223ab42d31d5\n",
      "\n",
      "Current status: DatasetFeaturization. Beginning to featurize the dataset.\n",
      "Current status: DatasetEvaluation. Gathering dataset statistics.\n",
      "Current status: FeaturesGeneration. Generating features for the dataset.\n",
      "Current status: DatasetFeaturizationCompleted. Completed featurizing the dataset.\n",
      "Current status: DatasetCrossValidationSplit. Generating individually featurized CV splits.\n",
      "\n",
      "****************************************************************************************************\n",
      "DATA GUARDRAILS: \n",
      "\n",
      "TYPE:         Missing values imputation\n",
      "STATUS:       FIXED\n",
      "DESCRIPTION:  The training data had the following missing values which were resolved. Please review your data source for data quality issues and possibly filter out the rows with these missing values. If the missing values are expected, you can either accept the above imputation, or implement your own custom imputation that may be more appropriate based on the data type and business process.\n",
      "PARAMETERS:   Column name : industry, Imputation type : mean\n",
      "              Column name : temp, Imputation type : mean\n",
      "              \n",
      "TYPE:         High cardinality feature detection\n",
      "STATUS:       PASSED\n",
      "DESCRIPTION:  Your inputs were analyzed, and no high cardinality features were detected.\n",
      "\n",
      "****************************************************************************************************\n",
      "Current status: ModelSelection. Beginning model selection.\n",
      "\n",
      "****************************************************************************************************\n",
      "ITERATION: The iteration being evaluated.\n",
      "PIPELINE: A summary description of the pipeline being evaluated.\n",
      "DURATION: Time taken for the current iteration.\n",
      "METRIC: The result of computing score on the fitted pipeline.\n",
      "BEST: The best observed score thus far.\n",
      "****************************************************************************************************\n",
      "\n",
      " ITERATION   PIPELINE                                       DURATION      METRIC      BEST\n",
      "         0   StandardScalerWrapper ElasticNet               0:00:15       0.0470    0.0470\n",
      "         1   StandardScalerWrapper ElasticNet               0:00:14       0.0470    0.0470\n",
      "         2   StandardScalerWrapper ElasticNet               0:00:18       0.0488    0.0470\n",
      "         3   StandardScalerWrapper ElasticNet               0:00:14       0.0732    0.0470\n",
      "         4   StandardScalerWrapper DecisionTree             0:00:17       0.0116    0.0116\n",
      "         5   MaxAbsScaler GradientBoosting                  0:00:17       0.0641    0.0116\n",
      "         6   MaxAbsScaler DecisionTree                      0:00:15       0.0144    0.0116\n",
      "         7   StandardScalerWrapper ElasticNet               0:00:17       0.0923    0.0116\n",
      "         8   MaxAbsScaler RandomForest                      0:00:17       0.0125    0.0116\n",
      "         9   MaxAbsScaler DecisionTree                      0:00:14       0.0141    0.0116\n",
      "        10   StandardScalerWrapper ElasticNet               0:00:15       0.0494    0.0116\n",
      "        11   StandardScalerWrapper ElasticNet               0:00:17       0.0470    0.0116\n",
      "        12   StandardScalerWrapper ElasticNet               0:00:15       0.0474    0.0116\n",
      "        13   MaxAbsScaler DecisionTree                      0:00:19       0.0160    0.0116\n",
      "        14   MaxAbsScaler ElasticNet                        0:00:14       0.0971    0.0116\n",
      "        15   StandardScalerWrapper ElasticNet               0:00:14       0.0479    0.0116\n",
      "        16   StandardScalerWrapper ElasticNet               0:00:17       0.0511    0.0116\n",
      "        17   StandardScalerWrapper DecisionTree             0:00:15       0.0150    0.0116\n",
      "        18   MaxAbsScaler DecisionTree                      0:00:17       0.0146    0.0116\n",
      "        19   MaxAbsScaler DecisionTree                      0:00:15       0.0122    0.0116\n",
      "        20   StandardScalerWrapper ElasticNet               0:00:17       0.0472    0.0116\n",
      "        21   StandardScalerWrapper ElasticNet               0:00:17       0.0472    0.0116\n",
      "        22   TruncatedSVDWrapper ElasticNet                 0:00:15       0.0513    0.0116\n",
      "        23   StandardScalerWrapper ElasticNet               0:00:18       0.0741    0.0116\n",
      "        24   StandardScalerWrapper ElasticNet               0:00:15       0.0461    0.0116\n",
      "        25   MaxAbsScaler DecisionTree                      0:00:16       0.0329    0.0116\n",
      "        26   TruncatedSVDWrapper ElasticNet                 0:00:17       0.0460    0.0116\n",
      "        27   StandardScalerWrapper ElasticNet               0:00:16       0.0477    0.0116\n",
      "        28   StandardScalerWrapper ElasticNet               0:00:16       0.0498    0.0116\n",
      "        29   MaxAbsScaler ElasticNet                        0:00:16       0.0607    0.0116\n",
      "        30   VotingEnsemble                                 0:00:23       0.0095    0.0095\n",
      "        31   StackEnsemble                                  0:00:27       0.0118    0.0095\n",
      "Stopping criteria reached at iteration 31. Ending experiment.\n",
      "Running on local machine\n",
      "Parent Run ID: AutoML_37851448-1fde-4a53-833b-50e348b10128\n",
      "\n",
      "Current status: DatasetFeaturization. Beginning to featurize the dataset.\n",
      "Current status: DatasetEvaluation. Gathering dataset statistics.\n",
      "Current status: FeaturesGeneration. Generating features for the dataset.\n",
      "Current status: DatasetFeaturizationCompleted. Completed featurizing the dataset.\n",
      "Current status: DatasetCrossValidationSplit. Generating individually featurized CV splits.\n",
      "\n",
      "****************************************************************************************************\n",
      "DATA GUARDRAILS: \n",
      "\n",
      "TYPE:         Missing values imputation\n",
      "STATUS:       FIXED\n",
      "DESCRIPTION:  The training data had the following missing values which were resolved. Please review your data source for data quality issues and possibly filter out the rows with these missing values. If the missing values are expected, you can either accept the above imputation, or implement your own custom imputation that may be more appropriate based on the data type and business process.\n",
      "PARAMETERS:   Column name : industry, Imputation type : mean\n",
      "              Column name : temp, Imputation type : mean\n",
      "              \n",
      "TYPE:         High cardinality feature detection\n",
      "STATUS:       PASSED\n",
      "DESCRIPTION:  Your inputs were analyzed, and no high cardinality features were detected.\n",
      "\n",
      "****************************************************************************************************\n",
      "Current status: ModelSelection. Beginning model selection.\n",
      "\n",
      "****************************************************************************************************\n",
      "ITERATION: The iteration being evaluated.\n",
      "PIPELINE: A summary description of the pipeline being evaluated.\n",
      "DURATION: Time taken for the current iteration.\n",
      "METRIC: The result of computing score on the fitted pipeline.\n",
      "BEST: The best observed score thus far.\n",
      "****************************************************************************************************\n",
      "\n",
      " ITERATION   PIPELINE                                       DURATION      METRIC      BEST\n",
      "         0   StandardScalerWrapper ElasticNet               0:00:14       0.0466    0.0466\n",
      "         1   StandardScalerWrapper ElasticNet               0:00:14       0.0464    0.0464\n",
      "         2   StandardScalerWrapper ElasticNet               0:00:17       0.0486    0.0464\n",
      "         3   StandardScalerWrapper ElasticNet               0:00:14       0.0736    0.0464\n",
      "         4   StandardScalerWrapper DecisionTree             0:00:15       0.0137    0.0137\n",
      "         5   MaxAbsScaler GradientBoosting                  0:00:15       0.0621    0.0137\n",
      "         6   MaxAbsScaler DecisionTree                      0:00:15       0.0163    0.0137\n",
      "         7   StandardScalerWrapper ElasticNet               0:00:14       0.0918    0.0137\n",
      "         8   MaxAbsScaler RandomForest                      0:00:14       0.0141    0.0137\n",
      "         9   MaxAbsScaler DecisionTree                      0:00:15       0.0157    0.0137\n",
      "        10   StandardScalerWrapper ElasticNet               0:00:15       0.0490    0.0137\n",
      "        11   StandardScalerWrapper ElasticNet               0:00:14       0.0465    0.0137\n",
      "        12   StandardScalerWrapper ElasticNet               0:00:14       0.0468    0.0137\n",
      "        13   MaxAbsScaler DecisionTree                      0:00:15       0.0169    0.0137\n",
      "        14   MaxAbsScaler ElasticNet                        0:00:14       0.0980    0.0137\n",
      "        15   StandardScalerWrapper ElasticNet               0:00:14       0.0473    0.0137\n",
      "        16   StandardScalerWrapper ElasticNet               0:00:14       0.0503    0.0137\n",
      "        17   StandardScalerWrapper DecisionTree             0:00:15       0.0124    0.0124\n",
      "        18   MaxAbsScaler DecisionTree                      0:00:14       0.0182    0.0124\n",
      "        19   MaxAbsScaler DecisionTree                      0:00:14       0.0194    0.0124\n",
      "        20   StandardScalerWrapper ElasticNet               0:00:14       0.0467    0.0124\n",
      "        21   StandardScalerWrapper ElasticNet               0:00:22       0.0467    0.0124\n",
      "        22   TruncatedSVDWrapper ElasticNet                 0:00:22       0.0508    0.0124\n",
      "        23   StandardScalerWrapper ElasticNet               0:00:18       0.0739    0.0124\n",
      "        24   StandardScalerWrapper ElasticNet               0:00:14       0.0455    0.0124\n",
      "        25   MaxAbsScaler DecisionTree                      0:00:18       0.0397    0.0124\n",
      "        26   TruncatedSVDWrapper ElasticNet                 0:00:15       0.0459    0.0124\n",
      "        27   StandardScalerWrapper ElasticNet               0:00:15       0.0473    0.0124\n",
      "        28   StandardScalerWrapper ElasticNet               0:00:15       0.0494    0.0124\n",
      "        29   MaxAbsScaler ElasticNet                        0:00:14       0.0605    0.0124\n",
      "        30   VotingEnsemble                                 0:00:22       0.0108    0.0108\n",
      "        31   StackEnsemble                                  0:00:22       0.0134    0.0108\n",
      "Stopping criteria reached at iteration 31. Ending experiment.\n",
      "Running on local machine\n",
      "Parent Run ID: AutoML_b3c9dbb3-4eee-4980-b842-599aea0555e7\n",
      "\n",
      "Current status: DatasetFeaturization. Beginning to featurize the dataset.\n",
      "Current status: DatasetEvaluation. Gathering dataset statistics.\n",
      "Current status: FeaturesGeneration. Generating features for the dataset.\n",
      "Current status: DatasetFeaturizationCompleted. Completed featurizing the dataset.\n",
      "Current status: DatasetCrossValidationSplit. Generating individually featurized CV splits.\n",
      "\n",
      "****************************************************************************************************\n",
      "DATA GUARDRAILS: \n",
      "\n",
      "TYPE:         Missing values imputation\n",
      "STATUS:       FIXED\n",
      "DESCRIPTION:  The training data had the following missing values which were resolved. Please review your data source for data quality issues and possibly filter out the rows with these missing values. If the missing values are expected, you can either accept the above imputation, or implement your own custom imputation that may be more appropriate based on the data type and business process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARAMETERS:   Column name : industry, Imputation type : mean\n",
      "              Column name : temp, Imputation type : mean\n",
      "              \n",
      "TYPE:         High cardinality feature detection\n",
      "STATUS:       PASSED\n",
      "DESCRIPTION:  Your inputs were analyzed, and no high cardinality features were detected.\n",
      "\n",
      "****************************************************************************************************\n",
      "Current status: ModelSelection. Beginning model selection.\n",
      "\n",
      "****************************************************************************************************\n",
      "ITERATION: The iteration being evaluated.\n",
      "PIPELINE: A summary description of the pipeline being evaluated.\n",
      "DURATION: Time taken for the current iteration.\n",
      "METRIC: The result of computing score on the fitted pipeline.\n",
      "BEST: The best observed score thus far.\n",
      "****************************************************************************************************\n",
      "\n",
      " ITERATION   PIPELINE                                       DURATION      METRIC      BEST\n",
      "         0   StandardScalerWrapper ElasticNet               0:00:13       0.0471    0.0471\n",
      "         1   StandardScalerWrapper ElasticNet               0:00:13       0.0470    0.0470\n",
      "         2   StandardScalerWrapper ElasticNet               0:00:14       0.0491    0.0470\n",
      "         3   StandardScalerWrapper ElasticNet               0:00:13       0.0736    0.0470\n",
      "         4   StandardScalerWrapper DecisionTree             0:00:13       0.0124    0.0124\n",
      "         5   MaxAbsScaler GradientBoosting                  0:00:14       0.0657    0.0124\n",
      "         6   MaxAbsScaler DecisionTree                      0:00:14       0.0143    0.0124\n",
      "         7   StandardScalerWrapper ElasticNet               0:00:13       0.0917    0.0124\n",
      "         8   MaxAbsScaler RandomForest                      0:00:14       0.0125    0.0124\n",
      "         9   MaxAbsScaler DecisionTree                      0:00:14       0.0192    0.0124\n",
      "        10   StandardScalerWrapper ElasticNet               0:00:13       0.0496    0.0124\n",
      "        11   StandardScalerWrapper ElasticNet               0:00:14       0.0470    0.0124\n",
      "        12   StandardScalerWrapper ElasticNet               0:00:13       0.0473    0.0124\n",
      "        13   MaxAbsScaler DecisionTree                      0:00:15       0.0180    0.0124\n",
      "        14   MaxAbsScaler ElasticNet                        0:00:14       0.0978    0.0124\n",
      "        15   StandardScalerWrapper ElasticNet               0:00:13       0.0476    0.0124\n",
      "        16   StandardScalerWrapper ElasticNet               0:00:14       0.0505    0.0124\n",
      "        17   StandardScalerWrapper DecisionTree             0:00:14       0.0124    0.0124\n",
      "        18   MaxAbsScaler DecisionTree                      0:00:15       0.0157    0.0124\n",
      "        19   MaxAbsScaler DecisionTree                      0:00:14       0.0118    0.0118\n",
      "        20   StandardScalerWrapper ElasticNet               0:00:15       0.0472    0.0118\n",
      "        21   StandardScalerWrapper ElasticNet               0:00:15       0.0472    0.0118\n",
      "        22   TruncatedSVDWrapper ElasticNet                 0:00:15       0.0513    0.0118\n",
      "        23   StandardScalerWrapper ElasticNet               0:00:15       0.0739    0.0118\n",
      "        24   StandardScalerWrapper ElasticNet               0:00:15       0.0471    0.0118\n",
      "        25   MaxAbsScaler DecisionTree                      0:00:15       0.0270    0.0118\n",
      "        26   TruncatedSVDWrapper ElasticNet                 0:00:15       0.0472    0.0118\n",
      "        27   StandardScalerWrapper ElasticNet               0:00:15       0.0478    0.0118\n",
      "        28   StandardScalerWrapper ElasticNet               0:00:16       0.0500    0.0118\n",
      "        29   MaxAbsScaler ElasticNet                        0:00:16       0.0625    0.0118\n",
      "        30   VotingEnsemble                                 0:00:24       0.0095    0.0095\n",
      "        31   StackEnsemble                                  0:00:25       0.0120    0.0095\n",
      "Stopping criteria reached at iteration 31. Ending experiment.\n",
      "Running on local machine\n",
      "Parent Run ID: AutoML_ffcb62ed-8587-4eac-8485-dde008330e90\n",
      "\n",
      "Current status: DatasetFeaturization. Beginning to featurize the dataset.\n",
      "Current status: DatasetEvaluation. Gathering dataset statistics.\n",
      "Current status: FeaturesGeneration. Generating features for the dataset.\n",
      "Current status: DatasetFeaturizationCompleted. Completed featurizing the dataset.\n",
      "Current status: DatasetCrossValidationSplit. Generating individually featurized CV splits.\n",
      "\n",
      "****************************************************************************************************\n",
      "DATA GUARDRAILS: \n",
      "\n",
      "TYPE:         Missing values imputation\n",
      "STATUS:       FIXED\n",
      "DESCRIPTION:  The training data had the following missing values which were resolved. Please review your data source for data quality issues and possibly filter out the rows with these missing values. If the missing values are expected, you can either accept the above imputation, or implement your own custom imputation that may be more appropriate based on the data type and business process.\n",
      "PARAMETERS:   Column name : industry, Imputation type : mean\n",
      "              Column name : temp, Imputation type : mean\n",
      "              \n",
      "TYPE:         High cardinality feature detection\n",
      "STATUS:       PASSED\n",
      "DESCRIPTION:  Your inputs were analyzed, and no high cardinality features were detected.\n",
      "\n",
      "****************************************************************************************************\n",
      "Current status: ModelSelection. Beginning model selection.\n",
      "\n",
      "****************************************************************************************************\n",
      "ITERATION: The iteration being evaluated.\n",
      "PIPELINE: A summary description of the pipeline being evaluated.\n",
      "DURATION: Time taken for the current iteration.\n",
      "METRIC: The result of computing score on the fitted pipeline.\n",
      "BEST: The best observed score thus far.\n",
      "****************************************************************************************************\n",
      "\n",
      " ITERATION   PIPELINE                                       DURATION      METRIC      BEST\n",
      "         0   StandardScalerWrapper ElasticNet               0:00:14       0.0492    0.0492\n",
      "         1   StandardScalerWrapper ElasticNet               0:00:17       0.0489    0.0489\n",
      "         2   StandardScalerWrapper ElasticNet               0:00:15       0.0507    0.0489\n",
      "         3   StandardScalerWrapper ElasticNet               0:00:14       0.0740    0.0489\n",
      "         4   StandardScalerWrapper DecisionTree             0:00:14       0.0133    0.0133\n",
      "         5   MaxAbsScaler GradientBoosting                  0:00:14       0.0655    0.0133\n",
      "         6   MaxAbsScaler DecisionTree                      0:00:14       0.0163    0.0133\n",
      "         7   StandardScalerWrapper ElasticNet               0:00:14       0.0917    0.0133\n",
      "         8   MaxAbsScaler RandomForest                      0:00:15       0.0150    0.0133\n",
      "         9   MaxAbsScaler DecisionTree                      0:00:18       0.0145    0.0133\n",
      "        10   StandardScalerWrapper ElasticNet               0:00:15       0.0513    0.0133\n",
      "        11   StandardScalerWrapper ElasticNet               0:00:14       0.0489    0.0133\n",
      "        12   StandardScalerWrapper ElasticNet               0:00:16       0.0491    0.0133\n",
      "        13   MaxAbsScaler DecisionTree                      0:00:15       0.0143    0.0133\n",
      "        14   MaxAbsScaler ElasticNet                        0:00:17       0.0980    0.0133\n",
      "        15   StandardScalerWrapper ElasticNet               0:00:14       0.0492    0.0133\n",
      "        16   StandardScalerWrapper ElasticNet               0:00:14       0.0522    0.0133\n",
      "        17   StandardScalerWrapper DecisionTree             0:00:21       0.0157    0.0133\n",
      "        18   MaxAbsScaler DecisionTree                      0:00:23       0.0151    0.0133\n",
      "        19   MaxAbsScaler DecisionTree                      0:00:21       0.0161    0.0133\n",
      "        20   StandardScalerWrapper ElasticNet               0:00:17       0.0491    0.0133\n",
      "        21   StandardScalerWrapper ElasticNet               0:00:15       0.0491    0.0133\n",
      "        22   TruncatedSVDWrapper ElasticNet                 0:00:16       0.0544    0.0133\n",
      "        23   StandardScalerWrapper ElasticNet               0:00:16       0.0746    0.0133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        24   StandardScalerWrapper ElasticNet               "
     ]
    }
   ],
   "source": [
    "automl_settings = {\n",
    "    \"iteration_timeout_minutes\": 4,\n",
    "    \"experiment_timeout_minutes\": 30,\n",
    "    \"enable_early_stopping\": True,\n",
    "    \"primary_metric\": 'normalized_mean_absolute_error',\n",
    "    \"featurization\": 'auto',\n",
    "    \"verbosity\": logging.INFO,\n",
    "    \"blacklist_models\": ['LightGBM'],\n",
    "    \"n_cross_validations\": 5,\n",
    "}\n",
    "\n",
    "\n",
    "for year in analysis_years:\n",
    "    train = data_test[~(data_test['year'] == year)].sort_values('datetime').reset_index(drop=True)\n",
    "    test = data_test[data_test['year'] == year].sort_values('datetime').reset_index(drop=True)\n",
    "    \n",
    "    automl_config = AutoMLConfig(task='regression',\n",
    "                             training_data=train,\n",
    "                             label_column_name=label,\n",
    "                             **automl_settings)\n",
    "\n",
    "    ws = Workspace.from_config()\n",
    "    experiment = Experiment(ws, \"EEB498\")\n",
    "    local_run = experiment.submit(automl_config, show_output=True)\n",
    "    \n",
    "    best_run, fitted_model = local_run.get_output()\n",
    "    \n",
    "    snapshot_directory='models/'\n",
    "    os.rename('model.pkl', f'model_{year}.pkl')\n",
    "    \n",
    "    preds = fitted_model.predict(test.loc[test['year']==year, test.columns != 'motiles'])\n",
    "    \n",
    "    locally_stored_runs[year] = {\n",
    "        'mse': metrics.mean_squared_error(test.loc[test['year']==year, 'motiles'].fillna(0), preds),\n",
    "        'mae': metrics.mean_absolute_error(test.loc[test['year']==year, 'motiles'].fillna(0), preds),\n",
    "        'predictions': preds,\n",
    "        'best_run': best_run,\n",
    "        'model': fitted_model,\n",
    "    }\n",
    "    \n",
    "    test_data = test_predictions_given_first_n_data(test.loc[test['year']==year, test.columns != 'motiles'], 25)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(fitted_model.predict(test_data))\n",
    "    ax.plot(test.loc[test['year']==2008, 'motiles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run, fitted_model = local_run.get_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = fitted_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_predictions_given_first_n_data(data, n):\n",
    "    test_data = data.copy()\n",
    "    test_data.iloc[25:, 1:6] = np.nan\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_info_raw = {2003: {'mse': 0.022721197546207945,\n",
    "  'mae': 0.11669384249363644,\n",
    "  'predictions': [0.03640573, 0.03640573, 0.04797311, 0.04602518, 0.08560173,\n",
    "         0.03640573, 0.04797311, 0.04797311, 0.00575422, 0.00569935,\n",
    "         0.00511387, 0.00611025, 0.00592927, 0.00318569, 0.00519563,\n",
    "         0.01978225, 0.04945153, 0.10346317, 0.27016504, 0.27624634,\n",
    "         0.48736403, 0.29852694, 0.29688699, 0.29698551, 0.30099863,\n",
    "         0.30097915, 0.30090223, 0.30082596, 0.30082596, 0.30082596,\n",
    "         0.30072661, 0.30082596, 0.30082596, 0.30090223, 0.30076532],\n",
    "    'train_mae': 0.0100\n",
    "  },\n",
    " 2004: {'mse': 9.53395073912808,\n",
    "  'mae': 2.0537649368805493,\n",
    "  'predictions': [0.01169341, 0.03676928, 0.01169341, 0.01169341, 0.01169341,\n",
    "         0.01169341, 0.01169341, 0.01169341, 0.01169341, 0.        ,\n",
    "         0.00474074, 0.00607407, 0.01766141, 0.00111111, 0.01026754,\n",
    "         0.06895492, 0.07699537, 0.14971893, 0.22905149, 0.2150387 ,\n",
    "         0.20940291, 0.22938707, 0.35869642, 0.27367115, 0.27367115,\n",
    "         0.28414198, 0.28745541, 0.28777879, 0.28777879, 0.28777879,\n",
    "         0.28777879, 0.28777879, 0.28777879, 0.28777879, 0.28777879],\n",
    "  'train_mae': 0.0258\n",
    "  },\n",
    " 2005: {'mse': 0.0995597420352555,\n",
    "  'mae': 0.23824512858026395,\n",
    "  'predictions': [0.04112419, 0.04112419, 0.04112419, 0.07702824, 0.08074587,\n",
    "         0.04112419, 0.04112419, 0.04112419, 0.04112419, 0.00553086,\n",
    "         0.00600315, 0.13667372, 0.13667372, 0.13667372, 0.00600315,\n",
    "         0.108798  , 0.19628494, 0.10761902, 0.16583831, 0.23864664,\n",
    "         0.11213282, 0.11819458, 0.12688071, 0.12887638, 0.12887638,\n",
    "         0.12887638, 0.23744059, 0.23744059, 0.23744059, 0.23744059,\n",
    "         0.23744059, 0.23744059, 0.23744059, 0.23744059, 0.23744059],\n",
    "  'train_mae': 0.0087\n",
    "  },\n",
    " 2006: {'mse': 0.008534451730800157,\n",
    "  'mae': 0.07047807606265853,\n",
    "  'predictions': [0.02793655, 0.02793655, 0.03144768, 0.03101417, 0.03175807,\n",
    "         0.02793655, 0.03144768, 0.03144768, 0.03101417, 0.00933691,\n",
    "         0.00056012, 0.00149959, 0.00055419, 0.00450837, 0.00542421,\n",
    "         0.00089625, 0.00192171, 0.01426724, 0.02381679, 0.11087303,\n",
    "         0.23891878, 0.38120575, 0.42819914, 0.39909043, 0.37124681,\n",
    "         0.43349284, 0.40507504, 0.43195341, 0.41041036, 0.40265457,\n",
    "         0.40235544, 0.40727347, 0.41032887, 0.41108709, 0.42282299],\n",
    "  'train_mae': 0.0100\n",
    "  },\n",
    " 2007: {'mse': 0.002753848817817705,\n",
    "  'mae': 0.04301749948252178,\n",
    "  'predictions': [0.0807715 , 0.0807715 , 0.0807715 , 0.0807715 , 0.0807715 ,\n",
    "         0.0807715 , 0.0807715 , 0.0807715 , 0.0807715 , 0.0807715 ,\n",
    "         0.0807715 , 0.0025641 , 0.        , 0.00459399, 0.03341766,\n",
    "         0.01181381, 0.02483105, 0.04993599, 0.05421498, 0.13639709,\n",
    "         0.23423848, 0.27381651, 0.23350108, 0.25184023, 0.27381651,\n",
    "         0.27646671, 0.29406847, 0.29406847, 0.27646671, 0.27646671,\n",
    "         0.27646671, 0.29406847, 0.27646671, 0.27646671, 0.27646671],\n",
    "  'train_mae': 0.0085\n",
    "  },\n",
    " 2008: {'mse': 0.12654851789455543,\n",
    "  'mae': 0.2300374734342253,\n",
    "  'predictions': [0.02756532, 0.02756532, 0.02756532, 0.02553229, 0.02553229,\n",
    "         0.02756532, 0.02756532, 0.02756532, 0.02553229, 0.02756532,\n",
    "         0.02756532, 0.02756532, 0.        , 0.        , 0.0025641 ,\n",
    "         0.03233333, 0.0701189 , 0.08104409, 0.04019049, 0.86228449,\n",
    "         1.03298801, 0.15724201, 0.09796512, 0.10048856, 0.10180116,\n",
    "         0.09371599, 0.10823446, 0.06505036, 0.07240196, 0.08203229,\n",
    "         0.08335383, 0.0830614 , 0.0858371 , 0.08238119, 0.08335383],\n",
    "  'train_mae': 0.0098\n",
    "  },\n",
    " 2009: {'mse': 0.04727254083834774,\n",
    "  'mae': 0.16443828106075478,\n",
    "  'predictions': [0.0446403 , 0.0446403 , 0.0446403 , 0.0446403 , 0.0446403 ,\n",
    "         0.0446403 , 0.0446403 , 0.0446403 , 0.0446403 , 0.0446403 ,\n",
    "         0.0446403 , 0.0446403 , 0.0446403 , 0.0446403 , 0.0446403 ,\n",
    "         0.07145043, 0.07145043, 0.10353403, 0.08411589, 0.13215198,\n",
    "         0.12601038, 0.41015909, 0.5653654 , 0.53092705, 0.52005395,\n",
    "         0.32957553, 0.3596909 , 0.3596909 , 0.3596909 , 0.32946057,\n",
    "         0.32946057, 0.3596909 , 0.3596909 , 0.32946057, 0.32946057],\n",
    "  'train_mae': 0.0095\n",
    "  },\n",
    " 2010: {'mse': 0.08107453762989339,\n",
    "  'mae': 0.19721581805594535,\n",
    "  'predictions': [0.01195157, 0.01278982, 0.02477208, 0.02477208, 0.02833271,\n",
    "         0.01335508, 0.01195157, 0.02477208, 0.02617559, 0.01195157,\n",
    "         0.01195157, 0.05482199, 0.        , 0.00231481, 0.00804725,\n",
    "         0.08325746, 0.12934278, 0.07593148, 0.10088178, 0.26642866,\n",
    "         0.24017608, 0.32613383, 0.51234308, 0.54139618, 0.53934487,\n",
    "         0.54070324, 0.5208475 , 0.51748235, 0.51686834, 0.53754881,\n",
    "         0.51359242, 0.51748235, 0.51748235, 0.53754881, 0.53754881],\n",
    "  'train_mae': 0.0108\n",
    "  },\n",
    " 2011: {'mse': 0.0012851783600649122,\n",
    "  'mae': 0.02150651338380984,\n",
    "  'predictions': [0.00035389, 0.00035389, 0.00035389, 0.00035389, 0.00035389,\n",
    "         0.00035389, 0.00035389, 0.00035389, 0.00035389, 0.00035389,\n",
    "         0.00035389, 0.00035389, 0.        , 0.        , 0.        ,\n",
    "         0.00226786, 0.03251735, 0.03033721, 0.08706567, 0.15978542,\n",
    "         0.09121399, 0.03612368, 0.06456116, 0.06456116, 0.07692788,\n",
    "         0.06541403, 0.05085621, 0.05085621, 0.05085621, 0.05085621,\n",
    "         0.05085621, 0.05085621, 0.05085621, 0.05085621, 0.05085621],\n",
    "  'train_mae': 0.0101\n",
    "  },\n",
    " 2012: {'mse': 0.01218804595330489,\n",
    "  'mae': 0.10000213591803112,\n",
    "  'predictions': [0.05019397, 0.05019397, 0.08116963, 0.08116963, 0.08116963,\n",
    "         0.05019397, 0.08116963, 0.08116963, 0.08116963, 0.05019397,\n",
    "         0.08116963, 0.08116963, 0.08116963, 0.02742857, 0.00028571,\n",
    "         0.08666125, 0.08666125, 0.04630621, 0.00918793, 0.02147259,\n",
    "         0.0346419 , 0.07571412, 0.01490221, 0.01894466, 0.01323386,\n",
    "         0.04756922, 0.04756922, 0.04205274, 0.04205274, 0.04205274,\n",
    "         0.04205274, 0.04205274, 0.04205274, 0.04205274, 0.04756922],\n",
    "  'train_mae': 0.0099\n",
    "  },\n",
    " 2013: {'mse': 0.006581112013569391,\n",
    "  'mae': 0.05170209983648096,\n",
    "  'predictions':[7.47836172e-03, 7.47836172e-03, 7.09454504e-03, 7.09454504e-03,\n",
    "         7.09454504e-03, 7.47836172e-03, 7.47836172e-03, 7.09454504e-03,\n",
    "         7.09454504e-03, 7.47836172e-03, 7.47836172e-03, 7.00672302e-03,\n",
    "         7.00672302e-03, 1.14587161e-03, 4.17320774e-04, 2.59811617e-04,\n",
    "         5.21453935e-02, 2.37334530e-02, 8.37937944e-02, 2.36763241e-01,\n",
    "         6.37776207e-02, 1.04482998e-01, 5.66379382e-02, 1.17893687e-01,\n",
    "         1.17477777e-01, 1.53282482e-01, 1.51077189e-01, 1.47524788e-01,\n",
    "         1.47524788e-01, 1.47524788e-01, 2.94062008e-01, 1.87770336e-01,\n",
    "         1.87770336e-01, 1.65023737e-01, 1.65023737e-01],\n",
    "  'train_mae': 0.0089\n",
    "  },\n",
    " 2014: {'mse': 0.007299844314156494,\n",
    "  'mae': 0.06327621598572274,\n",
    "  'predictions': [0.01113127, 0.01113127, 0.01222254, 0.01222254, 0.01230587,\n",
    "         0.02332571, 0.02441698, 0.02441698, 0.02499114, 0.02389987,\n",
    "         0.03323646, 0.03323646, 0.03321733, 0.02632829, 0.02966163,\n",
    "         0.0255937 , 0.0255021 , 0.0543577 , 0.07846889, 0.06443977,\n",
    "         0.06479549, 0.07233251, 0.02984042, 0.04310162, 0.02959191,\n",
    "         0.04672425, 0.02658566, 0.02658566, 0.02001161, 0.0199005 ,\n",
    "         0.02170474, 0.02060804, 0.02060804, 0.02179171, 0.02179171],\n",
    "  'train_mae': 0.0091\n",
    "  },\n",
    " 2015: {'mse': 0.4046104943945638,\n",
    "  'mae': 0.4272875017691028,\n",
    "  'predictions': [0.08519016, 0.08519016, 0.08519016, 0.08519016, 0.08519016,\n",
    "         0.08519016, 0.08519016, 0.08519016, 0.08519016, 0.08519016,\n",
    "         0.08519016, 0.08519016, 0.08519016, 0.08850768, 0.10685682,\n",
    "         0.11436559, 0.09932872, 0.14588505, 0.29183207, 0.29183207,\n",
    "         0.29183207, 0.18152179, 0.19215904, 0.05088115, 0.04855987,\n",
    "         0.06355432, 0.05924528, 0.05924528, 0.05924528, 0.05924528,\n",
    "         0.05924528, 0.05924528, 0.05924528, 0.05924528, 0.07438911],\n",
    "  'train_mae': 0.0110\n",
    "  },\n",
    " 2016: {'mse': 0.005374535087745641,\n",
    "  'mae': 0.05260032285635656,\n",
    "  'predictions': [0.01379038, 0.01461153, 0.01424419, 0.01424419, 0.01397097,\n",
    "         0.01393853, 0.01397097, 0.01424419, 0.01397097, 0.01311035,\n",
    "         0.01396394, 0.01358663, 0.01444117, 0.0166659 , 0.00451518,\n",
    "         0.08573733, 0.03842886, 0.02198434, 0.02451663, 0.05329424,\n",
    "         0.03659526, 0.03756506, 0.0316501 , 0.05083847, 0.03291896,\n",
    "         0.04692465, 0.07526995, 0.04317287, 0.07541578, 0.07618588,\n",
    "         0.04255767, 0.04178757, 0.04178757, 0.04178757, 0.04289074],\n",
    "  'train_mae': 0.0093\n",
    "  },\n",
    " 2017: {'mse': 0.007286320159917405,\n",
    "  'mae': 0.0660832121643928,\n",
    "  'predictions': [0.02001213, 0.02001213, 0.01596451, 0.01596451, 0.01596451,\n",
    "         0.02001213, 0.0266788 , 0.02263118, 0.02263118, 0.0266788 ,\n",
    "         0.0266788 , 0.02263118, 0.02263118, 0.00666667, 0.00763889,\n",
    "         0.02701867, 0.04080843, 0.06240109, 0.1189604 , 0.1259293 ,\n",
    "         0.15054271, 0.16950948, 0.18371927, 0.28778439, 0.22520711,\n",
    "         0.10197703, 0.09556783, 0.09556783, 0.09623449, 0.09623449,\n",
    "         0.09623449, 0.09556783, 0.09556783, 0.09623449, 0.09623449],\n",
    "  'train_mae': 0.0109\n",
    "  }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to json\n",
    "import json\n",
    "\n",
    "with open('sufficient_info.json', 'w') as fp:\n",
    "    json.dump(automl_info_raw, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EEB498",
   "language": "python",
   "name": "eeb498"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
